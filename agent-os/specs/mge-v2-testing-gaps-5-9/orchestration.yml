# MGE V2 Testing Gaps 5-9 - Orchestration Configuration

name: mge-v2-testing-gaps-5-9
version: 1.0.0
strategy: adaptive
priority: critical
category: test_coverage

# Execution Configuration
execution:
  mode: sequential_phases_parallel_tasks
  max_parallel_tasks: 3
  checkpoint_frequency: task_completion
  auto_validate: true
  stop_on_failure: true
  test_driven: true

# Phase Configuration
phases:
  - id: phase_1
    name: "Fix Infrastructure & Gap 7 Tests"
    duration_estimate: "4 hours"
    tasks: [1.1, 1.2, 1.3]
    dependencies: []
    validation_gate:
      - "Test collection working (no errors)"
      - "topological_sorter tests can execute"
      - "pytest infrastructure functional"

  - id: phase_2
    name: "Gap 5 - Dependency Graph Builder Tests"
    duration_estimate: "1 day"
    tasks: [2.1, 2.2, 2.3, 2.4, 2.5]
    dependencies: [phase_1]
    validation_gate:
      - "25+ tests passing"
      - "≥95% code coverage"
      - "Symbol extraction working"
      - "Dependency detection accurate"
      - "≥90% edge accuracy (integration test)"

  - id: phase_3
    name: "Gap 6 - Atomization API Tests"
    duration_estimate: "1 day"
    tasks: [3.1, 3.2, 3.3, 3.4]
    dependencies: [phase_2]
    validation_gate:
      - "30+ tests passing"
      - "≥95% code coverage"
      - "All 6 REST endpoints tested"
      - "CRUD operations verified"
      - "Error handling complete"

  - id: phase_4
    name: "Gap 7 - Topological Sorter Tests"
    duration_estimate: "1 day"
    tasks: [4.1, 4.2, 4.3, 4.4]
    dependencies: [phase_1]
    validation_gate:
      - "25+ tests passing"
      - "≥95% code coverage"
      - "Wave generation verified"
      - "Cycle breaking working"
      - "FAS algorithm validated"

  - id: phase_5
    name: "Gap 8 - Concurrency Controller Tests"
    duration_estimate: "1 day"
    tasks: [5.1, 5.2, 5.3, 5.4]
    dependencies: [phase_4]
    validation_gate:
      - "20+ tests passing"
      - "≥95% code coverage"
      - "Priority queue working"
      - "Backpressure functional"
      - "Statistics accurate"

  - id: phase_6
    name: "Gap 9 - Cost Guardrails Tests"
    duration_estimate: "1 day"
    tasks: [6.1, 6.2, 6.3, 6.4, 6.5]
    dependencies: [phase_5]
    validation_gate:
      - "25+ tests passing"
      - "≥95% code coverage"
      - "Soft limits working"
      - "Hard limits blocking"
      - "Alerting configured"

  - id: phase_7
    name: "Integration & Performance Validation"
    duration_estimate: "1 day"
    tasks: [7.1, 7.2, 7.3, 7.4, 7.5]
    dependencies: [phase_2, phase_3, phase_4, phase_5, phase_6]
    validation_gate:
      - "5 integration tests passing"
      - "5 performance benchmarks passing"
      - "125+ total tests passing (100% pass rate)"
      - "≥95% coverage across all components"
      - "Documentation complete"

# Quality Gates
quality_gates:
  unit_tests:
    required: true
    min_coverage: 95
    fail_on_error: true
    fast_execution: true  # <30s full suite

  integration_tests:
    required: true
    min_tests: 5
    run_after_phase: 7
    fail_on_error: true

  performance_tests:
    required: true
    targets:
      - metric: "dependency_graph_1000_atoms"
        max_value: 5
        unit: "seconds"
      - metric: "atomization_large_task"
        max_value: 10
        unit: "seconds"
      - metric: "topological_sort_1000_atoms"
        max_value: 2
        unit: "seconds"
      - metric: "queue_throughput"
        min_value: 1000
        unit: "requests_per_second"
      - metric: "cost_check_latency"
        max_value: 100
        unit: "milliseconds"

  code_quality:
    lint: true
    type_check: false  # Python tests don't require strict typing
    fail_on_warnings: false

# Progress Tracking
progress:
  update_frequency: task_completion
  use_todo_write: true
  checkpoint_on:
    - task_completion
    - phase_completion
    - validation_gate
    - test_milestone  # Every 25 tests

  metrics_to_track:
    - tasks_completed
    - tests_passing
    - code_coverage_percent
    - current_phase
    - time_elapsed
    - estimated_completion

# Testing Strategy
testing:
  framework: "pytest"
  async_support: true
  mocking: "unittest.mock"
  fixtures: "comprehensive"

  test_organization:
    unit_tests:
      location: "tests/unit/"
      naming: "test_<component>.py"
      total_target: 105

    api_tests:
      location: "tests/api/routers/"
      naming: "test_<router>.py"
      total_target: 30

    integration_tests:
      location: "tests/integration/"
      naming: "test_integration_<scenario>.py"
      total_target: 5

    performance_tests:
      location: "tests/performance/"
      naming: "test_perf_<component>.py"
      total_target: 5

  coverage:
    tool: "pytest-cov"
    target: 95
    report_format: ["html", "term"]
    exclude_patterns:
      - "*/tests/*"
      - "*/migrations/*"

# Component Targets
components:
  gap_5_dependencies:
    file: "src/dependency/graph_builder.py"
    test_file: "tests/unit/test_graph_builder.py"
    target_tests: 25
    target_coverage: 95

  gap_6_atomization:
    file: "src/api/routers/atomization.py"
    test_file: "tests/api/routers/test_atomization.py"
    target_tests: 30
    target_coverage: 95

  gap_7_topological:
    file: "src/dependency/topological_sorter.py"
    test_file: "tests/unit/test_topological_sorter.py"
    target_tests: 25
    target_coverage: 95

  gap_8_concurrency:
    file: "src/concurrency/backpressure_queue.py"
    test_file: "tests/unit/test_backpressure_queue.py"
    target_tests: 20
    target_coverage: 95

  gap_9_cost:
    file: "src/cost/cost_guardrails.py"
    test_file: "tests/unit/test_cost_guardrails.py"
    target_tests: 25
    target_coverage: 95

# Integration Points
integrations:
  existing_tests:
    - name: "MGE V2 Execution Tests"
      location: "tests/mge/v2/execution/"
      status: "passing"
      count: 84

    - name: "MGE V2 Caching Tests"
      location: "tests/mge/v2/caching/"
      status: "passing"
      count: 58

  dependencies:
    - component: "FastAPI TestClient"
      usage: "API endpoint testing"
    - component: "NetworkX"
      usage: "Graph testing utilities"
    - component: "asyncio"
      usage: "Async test support"

# Error Handling
error_handling:
  test_failures:
    action: "stop_and_report"
    retry: false
    diagnostic_detail: "full"

  coverage_below_target:
    action: "warning"
    continue: true
    report_gaps: true

  performance_benchmark_failure:
    action: "warning"
    continue: true
    document_deviation: true

# Documentation
documentation:
  testing_summary:
    path: "DOCS/MGE_V2/TESTING_SUMMARY.md"
    target_lines: 400
    sections:
      - "Test Coverage by Component"
      - "Performance Benchmarks"
      - "Execution Guide"
      - "Troubleshooting"

  test_execution_guide:
    path: "DOCS/MGE_V2/TEST_EXECUTION_GUIDE.md"
    target_lines: 200
    include_commands: true

  coverage_reports:
    path: "htmlcov/"
    format: "html"
    auto_open: false

# Success Criteria
success_criteria:
  must_have:
    - "125+ tests passing (100% pass rate)"
    - "≥95% coverage per component"
    - "0 test failures"
    - "5 integration tests passing"
    - "5 performance benchmarks passing"
    - "Full suite execution <30 seconds"
    - "Documentation complete"
    - "Checklist updated"

  nice_to_have:
    - "Coverage >97%"
    - "Performance exceeding targets"
    - "Additional edge case tests"
    - "Enhanced error messages"

# Resource Estimates
resources:
  total_duration: "5 days"
  total_tasks: 35
  total_tests: 125
  total_test_lines: 3500
  total_doc_lines: 600

# Validation Approach
validation:
  method: "Test-driven development with continuous validation"
  continuous_integration: false  # Manual execution
  automated_coverage: true
  manual_review: false

# Risk Mitigation
risk_mitigation:
  - risk: "Test collection errors"
    probability: "low"
    impact: "high"
    mitigation: "Phase 1 fixes infrastructure first"

  - risk: "Complex async mocking"
    probability: "medium"
    impact: "medium"
    mitigation: "Use pytest-asyncio, clear patterns"

  - risk: "Coverage gaps"
    probability: "low"
    impact: "high"
    mitigation: "Systematic code path testing"

  - risk: "Performance flakiness"
    probability: "low"
    impact: "medium"
    mitigation: "Isolated execution, stable benchmarks"
