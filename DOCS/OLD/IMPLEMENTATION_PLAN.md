# Implementation Plan: ApplicationIR Integration in PRODUCTION_MODE

**Objetivo**: Evolucionar PRODUCTION_MODE para usar ApplicationIR como input principal
**Duraci√≥n Estimada**: 1.5-2 semanas
**Score Target**: A+ (100/100)

---

## üìã Overview

### Estado Actual

```python
# PRODUCTION_MODE actual (src/services/code_generation_service.py:346-421)
if production_mode:
    # ‚ùå Usa spec_requirements directamente
    patterns = await self._retrieve_production_patterns(spec_requirements)
    files_dict = await self._compose_patterns(patterns, spec_requirements)
```

### Estado Objetivo

```python
# PRODUCTION_MODE evolucionado
if production_mode:
    # ‚úÖ Usa ApplicationIR
    patterns = await self._retrieve_production_patterns(app_ir)
    files_dict = await self._compose_patterns(patterns, app_ir)
```

### Beneficios

1. **ApplicationIR Activado**: Representaci√≥n intermedia normalizada en uso
2. **Multi-Stack Ready**: Foundation para Django, Node.js, Go generators
3. **Clean Architecture**: Separaci√≥n clara entre "qu√©" (IR) y "c√≥mo" (Generator)
4. **Simplificaci√≥n**: Un solo code path (deprecar USE_BACKEND_GENERATOR)

---

## üéØ Fase 1: Refactor Core (1 semana)

### Objetivo

Refactorizar `CodeGenerationService` y `ModularArchitectureGenerator` para aceptar `ApplicationIR` manteniendo compatibilidad backward.

### Tareas

#### 1.1 Refactor `_retrieve_production_patterns()` (2 d√≠as)

**Archivo**: `src/services/code_generation_service.py`

**Cambio Actual**:

```python
async def _retrieve_production_patterns(
    self,
    spec_requirements: SpecRequirements
) -> List[Pattern]:
    """Retrieve production-ready patterns from PatternBank."""
    # Extrae de spec_requirements:
    # - entities = spec_requirements.entities
    # - endpoints = spec_requirements.endpoints
    # - metadata = spec_requirements.metadata
    ...
```

**Cambio Propuesto**:

```python
async def _retrieve_production_patterns(
    self,
    app_ir: ApplicationIR,
    spec_requirements: Optional[SpecRequirements] = None  # Legacy support
) -> List[Pattern]:
    """
    Retrieve production-ready patterns from PatternBank using ApplicationIR.

    Args:
        app_ir: The application intermediate representation
        spec_requirements: [DEPRECATED] Legacy support, will be removed in v2.0

    Returns:
        List of production-ready Pattern objects
    """
    # Extrae de ApplicationIR:
    entities = app_ir.domain_model.entities
    endpoints = app_ir.api_model.endpoints
    infra_config = app_ir.infrastructure_model
    behavior_flows = app_ir.behavior_model.flows
    validation_rules = app_ir.validation_model.rules

    # Convert IR entities to PatternBank format
    pattern_entities = [
        self._ir_entity_to_pattern_entity(entity)
        for entity in entities
    ]

    pattern_endpoints = [
        self._ir_endpoint_to_pattern_endpoint(endpoint)
        for endpoint in endpoints
    ]

    # Retrieve patterns using converted data
    patterns = []

    # Core architecture patterns
    patterns.append(self.pattern_bank.get_pattern("modular_architecture"))
    patterns.append(self.pattern_bank.get_pattern("project_structure"))

    # Domain model patterns
    if pattern_entities:
        patterns.append(self.pattern_bank.get_pattern("sqlalchemy_models"))
        patterns.append(self.pattern_bank.get_pattern("pydantic_schemas"))

    # API patterns
    if pattern_endpoints:
        patterns.append(self.pattern_bank.get_pattern("fastapi_routers"))
        patterns.append(self.pattern_bank.get_pattern("crud_endpoints"))

    # Infrastructure patterns
    if infra_config.database:
        patterns.append(self.pattern_bank.get_pattern("database_config"))
    if infra_config.vector_db:
        patterns.append(self.pattern_bank.get_pattern("qdrant_integration"))
    if infra_config.graph_db:
        patterns.append(self.pattern_bank.get_pattern("neo4j_integration"))

    # Behavior patterns (NEW - previously unused)
    if behavior_flows:
        patterns.append(self.pattern_bank.get_pattern("state_machines"))
        patterns.append(self.pattern_bank.get_pattern("workflow_engine"))

    # Validation patterns (NEW - previously unused)
    if validation_rules:
        patterns.append(self.pattern_bank.get_pattern("input_validation"))
        patterns.append(self.pattern_bank.get_pattern("business_rules"))

    # Testing and deployment patterns
    patterns.append(self.pattern_bank.get_pattern("pytest_config"))
    patterns.append(self.pattern_bank.get_pattern("docker_compose"))

    return patterns

# Helper conversion methods
def _ir_entity_to_pattern_entity(self, entity: Entity) -> Dict[str, Any]:
    """Convert ApplicationIR Entity to PatternBank entity format."""
    return {
        "name": entity.name,
        "description": entity.description,
        "attributes": [
            {
                "name": attr.name,
                "data_type": attr.data_type.value,  # Enum to string
                "is_primary_key": attr.is_primary_key,
                "is_nullable": attr.is_nullable,
                "is_unique": attr.is_unique,
                "default_value": attr.default_value,
                "constraints": attr.constraints
            }
            for attr in entity.attributes
        ],
        "relationships": [
            {
                "name": rel.name,
                "type": rel.type.value,  # RelationshipType enum
                "target_entity": rel.target_entity,
                "foreign_key": rel.foreign_key
            }
            for rel in entity.relationships
        ]
    }

def _ir_endpoint_to_pattern_endpoint(self, endpoint: Endpoint) -> Dict[str, Any]:
    """Convert ApplicationIR Endpoint to PatternBank endpoint format."""
    return {
        "path": endpoint.path,
        "method": endpoint.method.value,  # HttpMethod enum to string
        "operation_id": endpoint.operation_id,
        "description": endpoint.description,
        "tags": endpoint.tags,
        "parameters": [
            {
                "name": param.name,
                "location": param.location.value,  # ParameterLocation enum
                "type": param.type,
                "required": param.required,
                "description": param.description
            }
            for param in endpoint.parameters
        ],
        "request_body": endpoint.request_body,
        "response": endpoint.response
    }
```

**Checklist**:
- [ ] Implementar `_retrieve_production_patterns(app_ir, spec_requirements=None)`
- [ ] Implementar `_ir_entity_to_pattern_entity()`
- [ ] Implementar `_ir_endpoint_to_pattern_endpoint()`
- [ ] Agregar deprecation warning si se usa `spec_requirements`
- [ ] Unit tests para conversiones IR ‚Üí PatternBank format
- [ ] Validar que retorna los mismos 27 patrones que antes

#### 1.2 Refactor `_compose_patterns()` (3 d√≠as)

**Archivo**: `src/services/code_generation_service.py`

**Cambio Actual**:

```python
async def _compose_patterns(
    self,
    patterns: List[Pattern],
    spec_requirements: SpecRequirements
) -> Dict[str, str]:
    """Compose patterns into final codebase."""
    # Usa spec_requirements directamente
    entities = spec_requirements.entities
    endpoints = spec_requirements.endpoints
    ...
```

**Cambio Propuesto**:

```python
async def _compose_patterns(
    self,
    patterns: List[Pattern],
    app_ir: ApplicationIR,
    spec_requirements: Optional[SpecRequirements] = None  # Legacy
) -> Dict[str, str]:
    """
    Compose patterns into final codebase using ApplicationIR.

    Args:
        patterns: List of Pattern objects from PatternBank
        app_ir: The application intermediate representation
        spec_requirements: [DEPRECATED] Legacy support

    Returns:
        Dict mapping file paths to file contents
    """
    files_dict = {}

    # Extract from ApplicationIR
    entities = app_ir.domain_model.entities
    endpoints = app_ir.api_model.endpoints
    infra = app_ir.infrastructure_model
    behavior_flows = app_ir.behavior_model.flows
    invariants = app_ir.behavior_model.invariants
    validation_rules = app_ir.validation_model.rules
    test_cases = app_ir.validation_model.test_cases

    # 1. Generate domain models (app/models/)
    models_pattern = next(p for p in patterns if p.name == "sqlalchemy_models")
    schemas_pattern = next(p for p in patterns if p.name == "pydantic_schemas")

    for entity in entities:
        # SQLAlchemy model
        model_file = self._generate_sqlalchemy_model(entity, models_pattern)
        files_dict[f"app/models/{entity.name.lower()}.py"] = model_file

        # Pydantic schema
        schema_file = self._generate_pydantic_schema(entity, schemas_pattern)
        files_dict[f"app/schemas/{entity.name.lower()}.py"] = schema_file

    # 2. Generate API endpoints (app/api/)
    router_pattern = next(p for p in patterns if p.name == "fastapi_routers")
    crud_pattern = next(p for p in patterns if p.name == "crud_endpoints")

    # Group endpoints by entity
    endpoints_by_entity = {}
    for endpoint in endpoints:
        entity_tag = endpoint.tags[0] if endpoint.tags else "general"
        if entity_tag not in endpoints_by_entity:
            endpoints_by_entity[entity_tag] = []
        endpoints_by_entity[entity_tag].append(endpoint)

    for entity_name, entity_endpoints in endpoints_by_entity.items():
        router_file = self._generate_fastapi_router(
            entity_name,
            entity_endpoints,
            router_pattern,
            crud_pattern
        )
        files_dict[f"app/api/v1/{entity_name.lower()}.py"] = router_file

    # 3. Generate infrastructure (app/core/, docker/)
    db_pattern = next(p for p in patterns if p.name == "database_config")
    db_config_file = self._generate_database_config(infra.database, db_pattern)
    files_dict["app/core/database.py"] = db_config_file

    if infra.vector_db:
        qdrant_pattern = next(p for p in patterns if p.name == "qdrant_integration")
        qdrant_file = self._generate_qdrant_config(infra.vector_db, qdrant_pattern)
        files_dict["app/core/vector_db.py"] = qdrant_file

    if infra.graph_db:
        neo4j_pattern = next(p for p in patterns if p.name == "neo4j_integration")
        neo4j_file = self._generate_neo4j_config(infra.graph_db, neo4j_pattern)
        files_dict["app/core/graph_db.py"] = neo4j_file

    # 4. Generate behavior logic (app/services/) - NEW
    if behavior_flows:
        workflow_pattern = next(
            (p for p in patterns if p.name == "workflow_engine"),
            None
        )
        if workflow_pattern:
            for flow in behavior_flows:
                workflow_file = self._generate_workflow(flow, workflow_pattern)
                files_dict[f"app/services/workflows/{flow.name.lower()}.py"] = workflow_file

    # 5. Generate validation logic (app/validators/) - NEW
    if validation_rules:
        validation_pattern = next(
            (p for p in patterns if p.name == "input_validation"),
            None
        )
        if validation_pattern:
            # Group validation rules by entity
            rules_by_entity = {}
            for rule in validation_rules:
                if rule.entity not in rules_by_entity:
                    rules_by_entity[rule.entity] = []
                rules_by_entity[rule.entity].append(rule)

            for entity_name, entity_rules in rules_by_entity.items():
                validator_file = self._generate_validator(
                    entity_name,
                    entity_rules,
                    validation_pattern
                )
                files_dict[f"app/validators/{entity_name.lower()}_validator.py"] = validator_file

    # 6. Generate tests (tests/) using test_cases from ValidationModel
    if test_cases:
        for test_case in test_cases:
            test_file = self._generate_test_from_case(test_case)
            files_dict[f"tests/{test_case.name.lower()}.py"] = test_file

    # 7. Generate Docker Compose with all infra services
    docker_pattern = next(p for p in patterns if p.name == "docker_compose")
    docker_file = self._generate_docker_compose(infra, docker_pattern)
    files_dict["docker-compose.yml"] = docker_file

    # 8. Generate documentation
    docs_pattern = next(p for p in patterns if p.name == "api_documentation")
    readme_file = self._generate_readme(app_ir, docs_pattern)
    files_dict["README.md"] = readme_file

    # 9. Generate deployment scripts
    scripts_pattern = next(p for p in patterns if p.name == "deployment_scripts")
    deploy_script = self._generate_deploy_script(infra, scripts_pattern)
    files_dict["scripts/deploy.sh"] = deploy_script

    return files_dict

# Helper generation methods (NEW - extract from ApplicationIR)
def _generate_sqlalchemy_model(self, entity: Entity, pattern: Pattern) -> str:
    """Generate SQLAlchemy model from ApplicationIR Entity."""
    # Implementation using pattern template
    ...

def _generate_pydantic_schema(self, entity: Entity, pattern: Pattern) -> str:
    """Generate Pydantic schema from ApplicationIR Entity."""
    ...

def _generate_fastapi_router(
    self,
    entity_name: str,
    endpoints: List[Endpoint],
    router_pattern: Pattern,
    crud_pattern: Pattern
) -> str:
    """Generate FastAPI router from ApplicationIR Endpoints."""
    ...

def _generate_workflow(self, flow: Flow, pattern: Pattern) -> str:
    """Generate workflow implementation from BehaviorModel Flow."""
    # NEW - previously BehaviorModel was unused
    ...

def _generate_validator(
    self,
    entity_name: str,
    rules: List[ValidationRule],
    pattern: Pattern
) -> str:
    """Generate validator from ValidationModel rules."""
    # NEW - previously ValidationModel was unused
    ...
```

**Checklist**:
- [ ] Implementar `_compose_patterns(patterns, app_ir, spec_requirements=None)`
- [ ] Implementar generadores para cada tipo de archivo:
  - [ ] `_generate_sqlalchemy_model()`
  - [ ] `_generate_pydantic_schema()`
  - [ ] `_generate_fastapi_router()`
  - [ ] `_generate_database_config()`
  - [ ] `_generate_qdrant_config()`
  - [ ] `_generate_neo4j_config()`
  - [ ] `_generate_workflow()` (NEW - usar BehaviorModel)
  - [ ] `_generate_validator()` (NEW - usar ValidationModel)
  - [ ] `_generate_test_from_case()` (NEW - usar ValidationModel.test_cases)
  - [ ] `_generate_docker_compose()`
  - [ ] `_generate_readme()`
  - [ ] `_generate_deploy_script()`
- [ ] Unit tests para cada generador
- [ ] Integration test: validar 57 archivos generados

#### 1.3 Update Main Generation Flow (1 d√≠a)

**Archivo**: `src/services/code_generation_service.py`

**Cambio en `generate_from_requirements()`**:

```python
async def generate_from_requirements(
    self,
    spec_requirements: SpecRequirements,
    context: Dict[str, Any] = None
) -> GeneratedCode:
    """Generate application code from requirements."""

    # ... (existing code) ...

    # Build ApplicationIR (Milestone 4) - ALWAYS construct IR
    from src.cognitive.ir.ir_builder import IRBuilder
    app_ir = IRBuilder.build_from_spec(spec_requirements)
    logger.info(f"ApplicationIR constructed: {app_ir.name} (ID: {app_ir.app_id})")

    # Persist Initial IR to Neo4j
    repo = Neo4jIRRepository()
    repo.save_application_ir(app_ir)
    repo.close()
    logger.info(f"‚úÖ ApplicationIR persisted to Neo4j: {app_ir.app_id}")  # ‚¨ÜÔ∏è Change to INFO

    # Check mode (DEPRECATE USE_BACKEND_GENERATOR in Phase 3)
    production_mode = os.getenv("PRODUCTION_MODE", "true").lower() == "true"

    if production_mode:
        # ‚úÖ NEW: Use ApplicationIR in PRODUCTION_MODE
        logger.info("üöÄ PRODUCTION_MODE enabled - using ApplicationIR + PatternBank")

        # Retrieve patterns using ApplicationIR
        patterns = await self._retrieve_production_patterns(app_ir)
        logger.info(f"Retrieved {len(patterns)} production patterns")

        # Compose patterns using ApplicationIR
        files_dict = await self._compose_patterns(patterns, app_ir)
        logger.info(f"Generated {len(files_dict)} files from ApplicationIR")

        # Package and return
        generated_code = GeneratedCode(
            files=files_dict,
            metadata={
                "generator": "ModularArchitectureGenerator",
                "patterns_used": len(patterns),
                "mode": "production",
                "ir_version": str(app_ir.app_id),  # Track IR version used
                "uses_application_ir": True  # ‚¨ÜÔ∏è NEW flag
            }
        )

        return generated_code

    else:
        # Legacy LLM-based generation (DEPRECATE in Phase 3)
        logger.warning("‚ö†Ô∏è PRODUCTION_MODE=false is deprecated. Use PRODUCTION_MODE=true.")
        # ... (existing legacy code) ...
```

**Checklist**:
- [ ] Update `generate_from_requirements()` to pass `app_ir` to pattern methods
- [ ] Change Neo4j persistence log from `logger.debug()` to `logger.info()`
- [ ] Add `uses_application_ir: True` flag to metadata
- [ ] Add `ir_version` tracking to metadata
- [ ] Add deprecation warning for `PRODUCTION_MODE=false`

#### 1.4 Validation (1 d√≠a)

**E2E Test Execution**:

```bash
# Run existing E2E test with refactored code
DATABASE_URL='postgresql+asyncpg://test:test@localhost/test' \
PRODUCTION_MODE=true \
timeout 180 python tests/e2e/real_e2e_full_pipeline.py
```

**Expected Results**:
- ‚úÖ 100% compliance maintained
- ‚úÖ 94%+ test pass rate maintained
- ‚úÖ 57 files generated
- ‚úÖ ApplicationIR used (check metadata flag `uses_application_ir: true`)
- ‚úÖ Neo4j persistence message visible (`logger.info()`)

**Validation Checklist**:
- [ ] E2E test passes with 100% compliance
- [ ] All 57 files generated correctly
- [ ] Neo4j persistence confirmed in logs
- [ ] ApplicationIR metadata present in GeneratedCode
- [ ] No regressions in file content quality
- [ ] Test pass rate ‚â•94%

---

## üîÑ Fase 2: Migrar Patrones (3-5 d√≠as)

### Objetivo

Actualizar los 27 patrones del PatternBank para trabajar con datos extra√≠dos de ApplicationIR.

### Tareas

#### 2.1 Audit Current Patterns (1 d√≠a)

**Crear inventario de patrones actuales**:

```python
# scripts/audit_patterns.py
from src.core.pattern_bank import PatternBank

pattern_bank = PatternBank()
patterns = pattern_bank.get_all_patterns()

print(f"Total patterns: {len(patterns)}")
for pattern in patterns:
    print(f"- {pattern.name}: {pattern.category}")
    print(f"  Input dependencies: {pattern.metadata.get('input_fields', [])}")
    print(f"  Requires refactor: {_check_if_needs_refactor(pattern)}")
```

**Expected Output**:

```
Total patterns: 27

Architecture Patterns:
- modular_architecture: core
  Input dependencies: ['project_name', 'entities', 'endpoints']
  Requires refactor: ‚úÖ Yes (uses spec_requirements.metadata)

- project_structure: core
  Input dependencies: ['project_name']
  Requires refactor: ‚ùå No (metadata only)

Domain Patterns:
- sqlalchemy_models: domain
  Input dependencies: ['entities[].fields']
  Requires refactor: ‚úÖ Yes (spec_requirements.entities)

- pydantic_schemas: domain
  Input dependencies: ['entities[].fields']
  Requires refactor: ‚úÖ Yes (spec_requirements.entities)

API Patterns:
- fastapi_routers: api
  Input dependencies: ['endpoints[].path', 'endpoints[].method']
  Requires refactor: ‚úÖ Yes (spec_requirements.endpoints)

Infrastructure Patterns:
- database_config: infrastructure
  Input dependencies: ['metadata.database_type']
  Requires refactor: ‚úÖ Yes (spec_requirements.metadata)

...
```

**Checklist**:
- [ ] List all 27 patterns
- [ ] Identify patterns that need refactor (estimate: 18-20 patterns)
- [ ] Identify patterns that don't need refactor (estimate: 7-9 patterns)
- [ ] Prioritize refactor order (critical ‚Üí nice-to-have)

#### 2.2 Refactor Core Patterns (2 d√≠as)

**Priority 1: Core Architecture Patterns**

1. **modular_architecture**
2. **project_structure**
3. **sqlalchemy_models**
4. **pydantic_schemas**
5. **fastapi_routers**
6. **crud_endpoints**

**Example Refactor: sqlalchemy_models**

**Before** (uses spec_requirements):

```python
# src/core/patterns/sqlalchemy_models.py
class SQLAlchemyModelsPattern(Pattern):
    def generate(self, spec_requirements: SpecRequirements) -> str:
        entities = spec_requirements.entities  # List[Entity from spec_parser]

        output = []
        for entity in entities:
            model_code = f"class {entity.name}(Base):\n"
            model_code += f"    __tablename__ = '{entity.name.lower()}'\n\n"

            for field in entity.fields:
                col_type = self._map_type(field.type)
                model_code += f"    {field.name} = Column({col_type}"
                if field.primary_key:
                    model_code += ", primary_key=True"
                if not field.required:
                    model_code += ", nullable=True"
                model_code += ")\n"

            output.append(model_code)

        return "\n\n".join(output)
```

**After** (uses ApplicationIR):

```python
# src/core/patterns/sqlalchemy_models.py
class SQLAlchemyModelsPattern(Pattern):
    def generate(self, entities: List[Entity]) -> str:
        """
        Generate SQLAlchemy models from ApplicationIR Entities.

        Args:
            entities: List of Entity objects from DomainModelIR

        Returns:
            Generated SQLAlchemy model code
        """
        output = []
        for entity in entities:  # Entity from application_ir.domain_model
            model_code = f"class {entity.name}(Base):\n"
            model_code += f'    """{ entity.description}"""\n'
            model_code += f"    __tablename__ = '{entity.name.lower()}'\n\n"

            for attr in entity.attributes:  # Attribute objects
                col_type = self._map_data_type(attr.data_type)  # DataType enum
                model_code += f"    {attr.name} = Column({col_type}"

                if attr.is_primary_key:
                    model_code += ", primary_key=True"
                if attr.is_nullable:
                    model_code += ", nullable=True"
                if attr.is_unique:
                    model_code += ", unique=True"
                if attr.default_value:
                    model_code += f", default={repr(attr.default_value)}"

                model_code += ")\n"

            # Add relationships (NEW - ApplicationIR includes relationships)
            for rel in entity.relationships:
                if rel.type == RelationshipType.ONE_TO_MANY:
                    model_code += f"    {rel.name} = relationship('{rel.target_entity}', back_populates='...')\n"
                elif rel.type == RelationshipType.MANY_TO_ONE:
                    model_code += f"    {rel.foreign_key} = Column(ForeignKey('{rel.target_entity}.id'))\n"
                    model_code += f"    {rel.name} = relationship('{rel.target_entity}')\n"

            output.append(model_code)

        return "\n\n".join(output)

    def _map_data_type(self, data_type: DataType) -> str:
        """Map ApplicationIR DataType enum to SQLAlchemy type."""
        mapping = {
            DataType.STRING: "String(255)",
            DataType.INTEGER: "Integer",
            DataType.FLOAT: "Float",
            DataType.BOOLEAN: "Boolean",
            DataType.DATETIME: "DateTime",
            DataType.UUID: "UUID",
            DataType.JSON: "JSON",
            DataType.TEXT: "Text"
        }
        return mapping.get(data_type, "String(255)")
```

**Checklist**:
- [ ] Refactor `modular_architecture` pattern
- [ ] Refactor `project_structure` pattern
- [ ] Refactor `sqlalchemy_models` pattern
- [ ] Refactor `pydantic_schemas` pattern
- [ ] Refactor `fastapi_routers` pattern
- [ ] Refactor `crud_endpoints` pattern
- [ ] Unit test each refactored pattern
- [ ] Integration test: generate files with refactored patterns

#### 2.3 Refactor Infrastructure & Testing Patterns (1-2 d√≠as)

**Priority 2: Infrastructure Patterns**

1. **database_config**
2. **qdrant_integration**
3. **neo4j_integration**
4. **docker_compose**
5. **observability_stack**

**Priority 3: Testing Patterns**

1. **pytest_config**
2. **unit_tests**
3. **integration_tests**
4. **e2e_tests**

**Example: docker_compose pattern**

**After** (uses InfrastructureModelIR):

```python
class DockerComposePattern(Pattern):
    def generate(self, infra: InfrastructureModelIR) -> str:
        """
        Generate docker-compose.yml from InfrastructureModelIR.

        Args:
            infra: Infrastructure configuration from ApplicationIR
        """
        services = {}

        # Always include main database
        services['postgres'] = {
            'image': f'postgres:{infra.database.version}',
            'environment': {
                'POSTGRES_DB': infra.database.name,
                'POSTGRES_USER': infra.database.user,
                'POSTGRES_PASSWORD': '${DB_PASSWORD}'
            },
            'ports': [f'{infra.database.port}:5432']
        }

        # Add vector DB if configured
        if infra.vector_db:
            services['qdrant'] = {
                'image': f'qdrant/qdrant:{infra.vector_db.version}',
                'ports': [f'{infra.vector_db.port}:6333']
            }

        # Add graph DB if configured
        if infra.graph_db:
            services['neo4j'] = {
                'image': f'neo4j:{infra.graph_db.version}',
                'ports': [
                    f'{infra.graph_db.port}:7687',
                    '7474:7474'
                ],
                'environment': {
                    'NEO4J_AUTH': '${NEO4J_USER}/${NEO4J_PASSWORD}'
                }
            }

        # Add observability stack if configured
        if infra.observability.prometheus_enabled:
            services['prometheus'] = {
                'image': 'prom/prometheus:latest',
                'ports': ['9090:9090']
            }

        if infra.observability.grafana_enabled:
            services['grafana'] = {
                'image': 'grafana/grafana:latest',
                'ports': ['3000:3000']
            }

        # Convert to YAML
        return yaml.dump({
            'version': infra.docker_compose_version,
            'services': services
        })
```

**Checklist**:
- [ ] Refactor all infrastructure patterns (5 patterns)
- [ ] Refactor all testing patterns (4 patterns)
- [ ] Unit test each pattern
- [ ] Integration test: full E2E with all patterns

#### 2.4 Add NEW Patterns for BehaviorModel & ValidationModel (1 d√≠a)

**NEW Patterns to Create**:

1. **workflow_engine**: Generate state machine workflows from BehaviorModel.flows
2. **business_rules**: Generate business rule validators from BehaviorModel.invariants
3. **input_validation**: Generate input validators from ValidationModel.rules
4. **test_case_generator**: Generate tests from ValidationModel.test_cases

**Example: workflow_engine pattern**

```python
class WorkflowEnginePattern(Pattern):
    """Generate workflow state machines from BehaviorModel flows."""

    def generate(self, flows: List[Flow]) -> Dict[str, str]:
        """
        Generate workflow implementations from BehaviorModel flows.

        Args:
            flows: List of Flow objects from BehaviorModelIR

        Returns:
            Dict mapping workflow names to implementation code
        """
        workflows = {}

        for flow in flows:
            if flow.type == FlowType.STATE_TRANSITION:
                code = self._generate_state_machine(flow)
            elif flow.type == FlowType.EVENT_DRIVEN:
                code = self._generate_event_handler(flow)
            elif flow.type == FlowType.SCHEDULED:
                code = self._generate_scheduled_task(flow)
            else:
                code = self._generate_generic_workflow(flow)

            workflows[flow.name] = code

        return workflows

    def _generate_state_machine(self, flow: Flow) -> str:
        """Generate state machine implementation."""
        return f"""
from enum import Enum
from typing import Optional

class {flow.name}State(Enum):
    {self._extract_states_from_flow(flow)}

class {flow.name}Workflow:
    \"\"\"{flow.description}\"\"\"

    def __init__(self):
        self.current_state = {flow.name}State.INITIAL

    def transition(self, event: str) -> bool:
        # State transition logic based on flow.steps
        {self._generate_transition_logic(flow)}
"""
```

**Checklist**:
- [ ] Create `workflow_engine` pattern
- [ ] Create `business_rules` pattern
- [ ] Create `input_validation` pattern
- [ ] Create `test_case_generator` pattern
- [ ] Add patterns to PatternBank registry
- [ ] Unit test each new pattern
- [ ] Integration test with BehaviorModel and ValidationModel

---

## üßπ Fase 3: Cleanup (2-3 d√≠as)

### Objetivo

Deprecar c√≥digo legacy y simplificar arquitectura a un solo code path.

### Tareas

#### 3.1 Deprecate USE_BACKEND_GENERATOR (1 d√≠a)

**Archivo**: `src/services/code_generation_service.py`

**Changes**:

```python
# Remove USE_BACKEND_GENERATOR initialization (lines 171-179)
# DELETE THIS BLOCK:
self.backend_generator = None
use_backend_generator = os.getenv("USE_BACKEND_GENERATOR", "false").lower() == "true"
if use_backend_generator:
    try:
        from src.services.fastapi_backend_generator import FastAPIBackendGenerator
        self.backend_generator = FastAPIBackendGenerator()
        logger.info("FastAPIBackendGenerator initialized (BackendGenerator interface)")
    except Exception as e:
        logger.warning(f"Could not initialize FastAPIBackendGenerator: {e}")

# Remove USE_BACKEND_GENERATOR path (lines 291-341)
# DELETE THIS BLOCK:
if self.backend_generator:
    files_dict = self.backend_generator.generate(app_ir, context)
    # ... rest of USE_BACKEND_GENERATOR code ...
```

**Add deprecation notice**:

```python
# At top of file
import warnings

# In __init__:
if os.getenv("USE_BACKEND_GENERATOR", "false").lower() == "true":
    warnings.warn(
        "USE_BACKEND_GENERATOR is deprecated and will be removed in v2.0. "
        "Use PRODUCTION_MODE=true instead.",
        DeprecationWarning,
        stacklevel=2
    )
```

**Checklist**:
- [ ] Add deprecation warning for USE_BACKEND_GENERATOR env var
- [ ] Remove `self.backend_generator` initialization
- [ ] Remove `if self.backend_generator:` code block
- [ ] Update documentation to remove USE_BACKEND_GENERATOR references
- [ ] Update tests to not use USE_BACKEND_GENERATOR
- [ ] Archive `src/services/backend_generator.py` (move to `archive/`)
- [ ] Archive `src/services/fastapi_backend_generator.py` (move to `archive/`)

#### 3.2 Deprecate PRODUCTION_MODE=false (1 d√≠a)

**Archivo**: `src/services/code_generation_service.py`

**Changes**:

```python
# Add strong deprecation warning
async def generate_from_requirements(
    self,
    spec_requirements: SpecRequirements,
    context: Dict[str, Any] = None
) -> GeneratedCode:

    production_mode = os.getenv("PRODUCTION_MODE", "true").lower() == "true"

    if not production_mode:
        # Raise error instead of warning
        raise DeprecationError(
            "PRODUCTION_MODE=false is deprecated and removed in v2.0. "
            "Use PRODUCTION_MODE=true (default). "
            "Legacy LLM-based generation is no longer supported."
        )

    # Only PRODUCTION_MODE=true code path remains
    # ... rest of implementation ...
```

**Alternative: Keep with strong warning**

```python
if not production_mode:
    logger.error("‚ùå PRODUCTION_MODE=false is DEPRECATED and will be REMOVED in v2.0")
    logger.error("Please migrate to PRODUCTION_MODE=true")
    # Allow to run but log extensively
    warnings.warn(
        "PRODUCTION_MODE=false will be removed in v2.0",
        DeprecationWarning,
        stacklevel=2
    )
```

**Checklist**:
- [ ] Add deprecation error/warning for PRODUCTION_MODE=false
- [ ] Update default to PRODUCTION_MODE=true in env examples
- [ ] Update documentation to remove PRODUCTION_MODE=false references
- [ ] Update tests to use PRODUCTION_MODE=true only
- [ ] Plan removal timeline (v2.0 target)

#### 3.3 Update Documentation (1 d√≠a)

**Files to Update**:

1. **README.md**
   - Remove USE_BACKEND_GENERATOR references
   - Remove PRODUCTION_MODE=false references
   - Update architecture diagrams

2. **DOCS/mvp/E2E_PIPELINE.md**
   - Update Phase 4: Code Generation to show ApplicationIR usage
   - Remove USE_BACKEND_GENERATOR mode explanation

3. **DOCS/mvp/GAPS_CLOSED_REPORT.md**
   - Update to A+ (100/100)
   - Remove "Building Blocks Future-Ready" section (now active)
   - Update debilidades to 0 puntos

4. **DOCS/mvp/ARCHITECTURE_DECISION.md**
   - Add "Implementation Complete" section
   - Document decision execution

**Example Update for GAPS_CLOSED_REPORT.md**:

```markdown
**Grade Final**: **A+ (100/100)** üéâ

#### Fortalezas (+100 puntos)
- ‚úÖ **Motor Cognitivo Completo** (+10)
- ‚úÖ **ApplicationIR Activo** (+10): Usado en PRODUCTION_MODE para generaci√≥n
- ‚úÖ **BehaviorModel Activo** (+5): Workflows y state machines generados
- ‚úÖ **ValidationModel Activo** (+5): Validators y test cases generados
- ‚úÖ **Arquitectura Simplificada** (+5): Un solo code path (PRODUCTION_MODE)
- ‚úÖ **57 Files Production-Ready** (+10)
- ‚úÖ **27 Patterns Refactored** (+5)
- ‚úÖ **100% E2E Compliance** (+10)

#### Debilidades
**Ninguna** - Todos los gaps cerrados ‚úÖ

### üéØ Logros Finales

1. **ApplicationIR Activado**: Usado en PRODUCTION_MODE para generaci√≥n
2. **BehaviorModel Activado**: Workflows y state machines generados desde flows
3. **ValidationModel Activado**: Validators y test cases generados desde rules
4. **Arquitectura Simplificada**: Un solo code path, legacy code deprecado
5. **Multi-Stack Ready**: Foundation lista para Django, Node.js, Go generators
```

**Checklist**:
- [ ] Update README.md
- [ ] Update E2E_PIPELINE.md
- [ ] Update GAPS_CLOSED_REPORT.md to A+ (100/100)
- [ ] Update ARCHITECTURE_DECISION.md with implementation results
- [ ] Update API documentation
- [ ] Update developer guide

#### 3.4 Final E2E Validation (1 d√≠a)

**Execute Full E2E Test**:

```bash
# Clean environment
docker-compose down -v
docker-compose up -d

# Run E2E test
DATABASE_URL='postgresql+asyncpg://test:test@localhost/test' \
PRODUCTION_MODE=true \
timeout 180 python tests/e2e/real_e2e_full_pipeline.py
```

**Validation Criteria**:

```yaml
compliance: 100%
test_pass_rate: ‚â•94%
files_generated: 57
uses_application_ir: true
uses_behavior_model: true
uses_validation_model: true
patterns_refactored: 27
legacy_code_paths: 0
```

**Checklist**:
- [ ] E2E test passes with 100% compliance
- [ ] All 57 files generated
- [ ] ApplicationIR used (check metadata)
- [ ] BehaviorModel used (workflows generated)
- [ ] ValidationModel used (validators generated)
- [ ] Test pass rate ‚â•94%
- [ ] No legacy code warnings
- [ ] Neo4j persistence confirmed
- [ ] Generated app runs successfully
- [ ] All tests in generated app pass

---

## üìä Progress Tracking

### Phase 1: Refactor Core (1 semana)

| Task | Status | Duration | Assignee |
|------|--------|----------|----------|
| 1.1 Refactor `_retrieve_production_patterns()` | ‚è≥ Pending | 2 days | - |
| 1.2 Refactor `_compose_patterns()` | ‚è≥ Pending | 3 days | - |
| 1.3 Update Main Generation Flow | ‚è≥ Pending | 1 day | - |
| 1.4 Validation | ‚è≥ Pending | 1 day | - |

### Phase 2: Migrar Patrones (3-5 d√≠as)

| Task | Status | Duration | Assignee |
|------|--------|----------|----------|
| 2.1 Audit Current Patterns | ‚è≥ Pending | 1 day | - |
| 2.2 Refactor Core Patterns | ‚è≥ Pending | 2 days | - |
| 2.3 Refactor Infrastructure & Testing | ‚è≥ Pending | 1-2 days | - |
| 2.4 Add NEW Patterns | ‚è≥ Pending | 1 day | - |

### Phase 3: Cleanup (2-3 d√≠as) ‚úÖ COMPLETED

| Task | Status | Duration | Assignee |
|------|--------|----------|----------|
| 3.1 Deprecate USE_BACKEND_GENERATOR | ‚úÖ Done | 1 day | 2025-11-23 |
| 3.2 Deprecate PRODUCTION_MODE=false | ‚úÖ Done | 1 day | 2025-11-23 |
| 3.3 Update Documentation | ‚úÖ Done | 1 day | 2025-11-23 |
| 3.4 Final E2E Validation | ‚è≥ Pending | 1 day | - |

---

## üéØ Success Metrics

### Technical Metrics

- [x] **ApplicationIR Usage**: 100% (used in PRODUCTION_MODE)
- [ ] **E2E Compliance**: 100%
- [ ] **Test Pass Rate**: ‚â•94%
- [ ] **Files Generated**: 57
- [ ] **Patterns Refactored**: 27/27
- [ ] **BehaviorModel Active**: Workflows generated
- [ ] **ValidationModel Active**: Validators generated
- [ ] **Code Paths**: 1 (PRODUCTION_MODE only)

### Quality Metrics

- [ ] **Code Coverage**: ‚â•85%
- [ ] **Documentation Coverage**: 100%
- [ ] **Deprecation Warnings**: 0 (all legacy removed)
- [ ] **E2E Test Duration**: <180s
- [ ] **Generated App Boot Time**: <5s

### Business Metrics

- [ ] **Grade**: A+ (100/100)
- [ ] **Multi-Stack Ready**: Foundation complete
- [ ] **Maintenance Complexity**: Reduced (1 path vs 3)
- [ ] **Developer Onboarding**: Simplified (clear architecture)

---

## üöÄ Next Steps After Implementation

### Immediate (Post-Implementation)

1. **Monitor Production Usage**
   - Track ApplicationIR usage metrics
   - Monitor E2E test stability
   - Validate generated app quality

2. **Gather Feedback**
   - Developer experience with refactored patterns
   - Performance impact of ApplicationIR
   - Identify optimization opportunities

### Short-term (1-2 months)

1. **Multi-Stack Support**
   - Create DjangoArchitectureGenerator
   - Create NodeExpressArchitectureGenerator
   - Share ApplicationIR as common contract

2. **Enhance BehaviorModel Usage**
   - More sophisticated workflow generation
   - State machine validation
   - Event-driven architecture support

3. **Enhance ValidationModel Usage**
   - Complex validation rules
   - Custom validators
   - Validation chaining

### Long-term (3-6 months)

1. **Learning System Activation**
   - Pattern quality scoring
   - Automatic pattern promotion
   - Feedback loop integration

2. **Pattern Ecosystem**
   - Community pattern contributions
   - Pattern marketplace
   - Pattern versioning and updates

3. **Advanced Code Generation**
   - Microservices architecture
   - Serverless deployments
   - Multi-cloud support

---

**Plan Created**: 2025-11-23
**Plan Status**: Ready for Execution
**Estimated Completion**: 2 weeks from start date
