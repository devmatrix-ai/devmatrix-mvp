# LLM Computational Architecture: MasterPlan System (MVP)

**Fecha**: 2025-10-20
**Autor**: Ariel Ghysels + Claude
**Tema**: An√°lisis computacional REALISTA para MVP
**Versi√≥n**: FINAL

---

## üéØ Objetivos del MVP

```python
MVP_GOALS = {
    "functional": "Genera c√≥digo que compila y funciona",
    "cost_target": "$10-15 per project (LLM API only)",
    "latency_target": "10-20 minutos",
    "success_rate": "70-80% (primera vez)",
    "scope": "Proyectos <50 tasks, complejidad standard"
}
```

---

## üìä Claude 4.x Family: Specs Verificadas

### Modelos Disponibles (Octubre 2025)

| Modelo | Context | Output | Input Cost | Output Cost | Best For |
|--------|---------|--------|------------|-------------|----------|
| **Opus 4.1** | 200K | 32K-64K | $15/M | $75/M | Critical architecture |
| **Sonnet 4.5** | 200K | 64K | $3/M | $15/M | Balanced (MVP default) |
| **Haiku 4.5** | 200K | 64K | $1/M | $5/M | Simple tasks, tests |

### Key Capabilities

1. **64K Output** ‚Üí MasterPlan completo cabe en una respuesta ‚úÖ
2. **Prompt Caching** ‚Üí 90% discount en repeated context ‚úÖ
3. **200K Context** ‚Üí Contexto generoso por task ‚úÖ
4. **Haiku con Reasoning** ‚Üí Ya no es "dumb and fast" ‚úÖ

---

## üî• GAME CHANGER: Prompt Caching

**Esto cambia TODO el an√°lisis de costos:**

```python
# Anthropic Prompt Caching (live desde hace meses)
PROMPT_CACHING = {
    "cache_write": "$3.75/M",  # Primera vez (25% premium)
    "cache_read": "$0.30/M",   # 90% DISCOUNT
    "cache_ttl": "5 minutes",  # Auto-refresh
    "max_cache": "200K tokens" # M√°ximo cacheable
}

# En nuestro sistema:
CACHEABLE_CONTEXT = {
    "discovery_document": 8_000,      # Se repite en 50 tasks
    "rag_examples": 20_000,           # Se repite en tasks similares
    "project_structure": 2_000,       # Se actualiza pero mayormente estable
    "system_prompt": 3_000,           # Id√©ntico siempre
    "total_cacheable": 33_000         # 33K tokens cacheables
}

# COSTO SIN CACHING (50 tasks):
WITHOUT_CACHING = {
    "50_tasks √ó 50K_input": 2_500_000,
    "cost": 2_500_000 √ó $3/M = "$7.50"
}

# COSTO CON CACHING:
WITH_CACHING = {
    "first_task_write": 33_000 √ó $3.75/M = "$0.124",
    "first_task_uncached": 17_000 √ó $3/M = "$0.051",
    "next_49_tasks_cached": 49 √ó 33_000 √ó $0.30/M = "$0.485",
    "next_49_tasks_uncached": 49 √ó 17_000 √ó $3/M = "$2.499",
    "total": "$3.16 (vs $7.50 = 58% SAVINGS)"
}
```

**CR√çTICO**: Prompt caching reduce costos de ejecuci√≥n **58%**. Debemos implementarlo desde d√≠a 1.

---

## üßÆ Token Estimation: Realista

### Phase 0: Discovery (3 passes)

```python
DISCOVERY_ESTIMATION = {
    # Pase 1: Initial analysis
    "pass_1": {
        "input": 500,    # User request + system prompt
        "output": 500,   # Questions + initial entities
        "model": "Sonnet 4.5",
        "cost": "$0.010"
    },

    # Pase 2: User answers (0 tokens LLM)
    "pass_2": "Interactive - no LLM cost",

    # Pase 3: DDD Modeling
    "pass_3": {
        "input": 2_500,  # Request + answers + DDD prompt
        "output": 5_000, # Full DDD model
        "model": "Sonnet 4.5",
        "cost": "$0.083"
    },

    "total_cost": "$0.093"
}
```

**Discovery**: ~$0.09 (Sonnet) o $0.40 (Opus para dominios complejos)

### Phase 1: RAG Retrieval

```python
RAG_RETRIEVAL = {
    "chromadb_queries": 10,     # Semantic search
    "llm_cost": "$0.00",        # No LLM, solo ChromaDB
    "latency": "1-2 segundos"
}
```

### Phase 2: MasterPlan Generation (Monolithic)

```python
MASTERPLAN_GENERATION = {
    "input": {
        "discovery": 8_000,
        "rag_learnings": 7_500,
        "system_prompt": 3_000,
        "json_schema": 1_000,
        "examples": 2_000,
        "total": 21_500
    },

    "output": {
        "phases": 1_000,
        "milestones": 2_000,
        "tasks_50": 10_000,
        "subtasks": 3_000,
        "dependencies": 1_000,
        "total": 17_000  # ‚úÖ Cabe en 64K
    },

    "model": "Sonnet 4.5",
    "cost": (21_500 √ó $3 + 17_000 √ó $15) / 1M = "$0.32"
}
```

**MasterPlan**: $0.32 (una sola llamada, aprovecha 64K output)

### Phase 3: Task Execution (50 tasks con Caching)

```python
TASK_EXECUTION = {
    "context_per_task": {
        "task_definition": 1_000,
        "dependencies": 5_000,
        "recent_tasks": 6_000,
        "phase_summary": 2_000,
        "project_structure": 2_000,
        "rag_examples": 20_000,       # ‚Üê Cacheable
        "available_code": 5_000,
        "database_schema": 3_000,     # ‚Üê Cacheable
        "environment": 1_000,
        "system_prompt": 3_000,       # ‚Üê Cacheable
        "total": 48_000,
        "cacheable": 26_000           # 54% cacheable
    },

    "output_per_task": 5_000,         # C√≥digo + tests + docs

    # Distribuci√≥n h√≠brida para MVP
    "distribution": {
        "haiku": 30,   # 60% - Simple/medium tasks
        "sonnet": 20,  # 40% - Complex tasks
        "opus": 0      # 0% - MVP no usa Opus
    },

    # C√°lculo con caching
    "haiku_tasks": {
        "first_uncached": 1 √ó (22K √ó $1 + 26K √ó $1.25 + 5K √ó $5) / 1M = "$0.080",
        "next_29_cached": 29 √ó (22K √ó $1 + 26K √ó $0.30 + 5K √ó $5) / 1M = "$5.27",
        "total": "$5.35"
    },

    "sonnet_tasks": {
        "first_uncached": 1 √ó (22K √ó $3 + 26K √ó $3.75 + 5K √ó $15) / 1M = "$0.240",
        "next_19_cached": 19 √ó (22K √ó $3 + 26K √ó $0.30 + 5K √ó $15) / 1M = "$3.93",
        "total": "$4.17"
    },

    "total_execution": "$9.52"
}
```

### Phase 4: Summaries & Retries

```python
SUMMARIES_RETRIES = {
    "phase_summaries": {
        "count": 5,
        "model": "Haiku",
        "cost": "$0.05"
    },

    "retries_realistic": {
        "fail_rate": "20%",           # 10 tasks fail
        "retry_cost": "$9.52 √ó 0.20 = $1.90"
    },

    "total": "$1.95"
}
```

---

## üí∞ COSTO TOTAL MVP (50 tasks, realistic)

```python
MVP_COST_BREAKDOWN = {
    "discovery": "$0.09",
    "rag_retrieval": "$0.00",
    "masterplan_generation": "$0.32",
    "task_execution_50": "$9.52",     # Con prompt caching
    "summaries": "$0.05",
    "retries_20%": "$1.90",

    "TOTAL_MVP": "$11.88"
}

# Breakdown por modelo
MODEL_DISTRIBUTION = {
    "haiku_60%": "$5.35 + $1.14 (retries) = $6.49",
    "sonnet_40%": "$0.09 + $0.32 + $4.17 + $0.76 (retries) = $5.34",
    "opus_0%": "$0.00"
}

# Comparaci√≥n
COMPARISON = {
    "sin_caching": "$17.50",
    "con_caching": "$11.88",
    "savings": "$5.62 (32% reduction)"
}
```

### Alternativa All-Sonnet (conservative)

```python
ALL_SONNET_MVP = {
    "discovery": "$0.09",
    "masterplan": "$0.32",
    "execution_50": "$11.50",         # Todos Sonnet, con caching
    "retries_20%": "$2.30",

    "total": "$14.21"
}
```

### Alternativa All-Haiku (ultra-budget)

```python
ALL_HAIKU_MVP = {
    "discovery": "$0.03",
    "masterplan": "$0.11",
    "execution_50": "$6.00",          # Todos Haiku, con caching
    "retries_30%": "$1.80",           # +10% fail rate con Haiku

    "total": "$7.94"
}
```

---

## ‚è±Ô∏è LATENCIA REALISTA

### Rate Limits por Tier

```python
ANTHROPIC_RATE_LIMITS = {
    "tier_1": {
        "rpm": 50,
        "tpm": 40_000,
        "description": "Nuevo usuario"
    },
    "tier_2": {
        "rpm": 1_000,
        "tpm": 400_000,
        "description": "$5+ spend, 7 days"
    },
    "tier_4": {
        "rpm": 4_000,
        "tpm": 4_000_000,
        "description": "$1,000+ spend"
    }
}
```

### Latencia por Scenario

```python
LATENCY_SCENARIOS = {
    "tier_1_sequential": {
        "discovery": "10s",
        "masterplan": "30s",
        "tasks_50_sequential": "50 √ó 25s = 1,250s = 21min",
        "retries_10": "10 √ó 25s = 250s = 4min",
        "total": "25-30 minutos"
    },

    "tier_2_parallel_2x": {
        "discovery": "10s",
        "masterplan": "30s",
        "tasks_50_parallel_2": "25 batches √ó 30s = 750s = 12.5min",
        "retries_10": "5 batches √ó 30s = 150s = 2.5min",
        "total": "15-18 minutos"
    },

    "tier_4_parallel_5x": {
        "discovery": "10s",
        "masterplan": "30s",
        "tasks_50_parallel_5": "10 batches √ó 35s = 350s = 6min",
        "retries_10": "2 batches √ó 35s = 70s = 1min",
        "total": "7-10 minutos"
    }
}
```

**MVP TARGET**: Tier 2 (~$100 spend) ‚Üí **15-18 minutos** por proyecto

---

## üéØ ESTRATEGIA MVP SIMPLIFICADA

### 1. Model Selection (Simple)

```python
def select_model_mvp(task: Task) -> str:
    """
    MVP: Solo Haiku + Sonnet
    Opus queda para v2
    """
    if task.task_type == "discovery":
        return "sonnet-4.5"  # Siempre Sonnet para discovery

    if task.task_type == "masterplan_generation":
        return "sonnet-4.5"  # Siempre Sonnet para MasterPlan

    if task.complexity in ["low", "medium"]:
        return "haiku-4.5"   # 60% de tasks

    return "sonnet-4.5"      # 40% de tasks (complex/high)
```

### 2. Prompt Caching Strategy

```python
class PromptCacheManager:
    """
    Gesti√≥n de prompt caching para MVP.
    """

    def build_cached_prompt(
        self,
        task: Task,
        cacheable_context: dict,
        variable_context: dict
    ) -> str:
        """
        Estructura optimizada para caching:

        [CACHEABLE SECTION] ‚Üê Anthropic cachea esto
        - System prompt (3K)
        - Discovery document (8K)
        - RAG examples (20K)
        - Database schema (3K)
        - Total: 34K cacheable

        [VARIABLE SECTION] ‚Üê Nuevo cada vez
        - Task definition (1K)
        - Dependencies espec√≠ficas (5K)
        - Recent tasks (6K)
        - Total: 12K variable
        """

        prompt = ""

        # Cacheable section (marca especial)
        prompt += "<cacheable>\n"
        prompt += f"System: {cacheable_context['system_prompt']}\n"
        prompt += f"Discovery: {cacheable_context['discovery']}\n"
        prompt += f"RAG: {cacheable_context['rag_examples']}\n"
        prompt += f"Schema: {cacheable_context['db_schema']}\n"
        prompt += "</cacheable>\n\n"

        # Variable section
        prompt += f"Task: {variable_context['task']}\n"
        prompt += f"Dependencies: {variable_context['dependencies']}\n"
        prompt += f"Recent: {variable_context['recent_tasks']}\n"

        return prompt

    async def generate_with_caching(
        self,
        prompt: str,
        model: str
    ):
        """
        Usa API de Anthropic con cache markers.
        """
        response = await self.anthropic.messages.create(
            model=model,
            messages=[{
                "role": "user",
                "content": prompt
            }],
            # CR√çTICO: Habilitar prompt caching
            system=[{
                "type": "text",
                "text": cacheable_section,
                "cache_control": {"type": "ephemeral"}  # ‚Üê Cache marker
            }]
        )

        # Log cache performance
        logger.info(f"Cache stats: {response.usage}")
        # usage.cache_creation_input_tokens: tokens escritos a cache
        # usage.cache_read_input_tokens: tokens le√≠dos de cache

        return response
```

### 3. Sequential Execution (MVP)

```python
class SequentialExecutor:
    """
    MVP: Sequential execution

    Evita race conditions.
    M√°s simple de implementar.
    Suficientemente r√°pido para MVP.
    """

    async def execute_masterplan(
        self,
        masterplan: MasterPlan
    ):
        """
        Ejecuta tasks secuencialmente con dependency awareness.
        """
        completed_tasks = set()

        while not masterplan.is_complete():
            # Get next executable tasks (no dependencies pendientes)
            executable = await self.get_executable_tasks(
                masterplan,
                completed_tasks
            )

            if not executable:
                logger.error("No executable tasks but plan incomplete - circular deps?")
                break

            # Execute ONE task at a time (sequential)
            task = executable[0]

            try:
                result = await self.execute_task(task)
                completed_tasks.add(task.task_id)

                # Update scratchpad
                await self.scratchpad.update(task, result)

                # Broadcast progress
                await self.broadcast_progress({
                    "completed": len(completed_tasks),
                    "total": len(masterplan.tasks),
                    "current": task.title
                })

            except Exception as e:
                logger.error(f"Task {task.task_id} failed: {e}")

                # Retry once
                if task.retry_count < 1:
                    task.retry_count += 1
                    logger.info(f"Retrying task {task.task_id}")
                    # Put back in queue
                    continue
                else:
                    # Mark failed, continue with other tasks
                    task.status = "failed"
                    await task.save()

        return masterplan
```

**MVP: Sequential OK** - 15-18 min es aceptable. Parallel execution para v2.

### 4. Validation Layers

```python
class ValidationManager:
    """
    Validaciones m√≠nimas para MVP.
    """

    async def validate_masterplan(
        self,
        masterplan_json: dict
    ) -> tuple[bool, List[str]]:
        """
        Valida MasterPlan generado por LLM.
        """
        errors = []

        # 1. JSON structure validation
        required_fields = ["phases", "milestones", "tasks"]
        for field in required_fields:
            if field not in masterplan_json:
                errors.append(f"Missing field: {field}")

        # 2. Tasks have required fields
        for task in masterplan_json.get("tasks", []):
            if not task.get("title"):
                errors.append(f"Task missing title: {task.get('task_id')}")
            if not task.get("description"):
                errors.append(f"Task missing description: {task.get('task_id')}")

        # 3. Dependencies exist
        task_ids = {t["task_id"] for t in masterplan_json.get("tasks", [])}
        for task in masterplan_json.get("tasks", []):
            for dep_id in task.get("depends_on", []):
                if dep_id not in task_ids:
                    errors.append(f"Task {task['task_id']} depends on non-existent {dep_id}")

        # 4. No circular dependencies (Neo4j check)
        cycles = await self.neo4j.detect_cycles(masterplan_json)
        if cycles:
            errors.append(f"Circular dependencies found: {cycles}")

        return len(errors) == 0, errors

    async def validate_task_output(
        self,
        task: Task,
        output: dict
    ) -> tuple[bool, List[str]]:
        """
        Valida output de task execution.
        """
        errors = []

        # 1. Check required fields
        if "files_created" not in output:
            errors.append("Missing files_created in output")

        # 2. Basic syntax check (Python/JS)
        for file_info in output.get("files_created", []):
            filepath = file_info.get("path")
            content = file_info.get("content")

            if filepath.endswith(".py"):
                # Check Python syntax
                try:
                    compile(content, filepath, "exec")
                except SyntaxError as e:
                    errors.append(f"Python syntax error in {filepath}: {e}")

            elif filepath.endswith(".js"):
                # Basic JS check (just look for common errors)
                if "import from " in content:  # Should be "import ... from"
                    errors.append(f"JS syntax error in {filepath}")

        # 3. Check files were actually created
        for file_info in output.get("files_created", []):
            if not await self.workspace.file_exists(file_info["path"]):
                errors.append(f"File not created: {file_info['path']}")

        return len(errors) == 0, errors
```

### 5. Error Handling con Retry

```python
class RetryManager:
    """
    Retry strategy para MVP.
    """

    async def execute_with_retry(
        self,
        task: Task,
        max_retries: int = 1  # MVP: solo 1 retry
    ):
        """
        Ejecuta task con retry si falla.
        """
        last_error = None

        for attempt in range(max_retries + 1):
            try:
                # Build context
                context = await self.context_builder.get_context_for_task(task)

                # Execute con modelo apropiado
                model = self.model_selector.select_model_mvp(task)
                result = await self.llm.generate_with_caching(
                    prompt=self.build_task_prompt(task, context),
                    model=model
                )

                # Parse result
                output = json.loads(result)

                # Validate
                valid, errors = await self.validator.validate_task_output(
                    task, output
                )

                if not valid:
                    raise ValidationError(f"Output validation failed: {errors}")

                # Success!
                return output

            except Exception as e:
                last_error = e
                logger.error(f"Task {task.task_id} attempt {attempt+1} failed: {e}")

                if attempt < max_retries:
                    # Include error feedback en pr√≥ximo intento
                    task.metadata["previous_error"] = str(e)
                    await asyncio.sleep(2)  # Brief pause
                    continue
                else:
                    # Final failure
                    raise TaskExecutionError(
                        f"Task failed after {max_retries+1} attempts: {last_error}"
                    )
```

---

## üéØ MVP SUCCESS METRICS

```python
MVP_METRICS = {
    "cost_per_project": {
        "target": "$12",
        "acceptable": "$10-15",
        "alarm": ">$20"
    },

    "latency": {
        "target": "15 minutos",
        "acceptable": "10-20 minutos",
        "alarm": ">30 minutos"
    },

    "success_rate": {
        "target": "75%",
        "acceptable": "70-80%",
        "definition": "Code compiles + basic tests pass"
    },

    "cache_hit_rate": {
        "target": "60%",
        "acceptable": "50-70%",
        "benefit": "32% cost reduction"
    },

    "user_satisfaction": {
        "target": "4/5 stars",
        "acceptable": "3.5+/5",
        "measure": "Post-project survey"
    }
}
```

---

## üöÄ MVP IMPLEMENTATION ROADMAP

### Week 1-2: Core Infrastructure

```python
WEEK_1_2 = {
    "llm_integration": [
        "Anthropic SDK integration",
        "Prompt caching implementation",
        "Model selector (Haiku/Sonnet)",
        "Rate limit handling",
        "Error handling & retries"
    ],

    "storage": [
        "PostgreSQL setup (MasterPlans, Tasks)",
        "Redis setup (session cache)",
        "ChromaDB setup (RAG storage)",
        "Neo4j setup (dependency graph)"
    ],

    "deliverable": "LLM calls working with caching"
}
```

### Week 3-4: Discovery & MasterPlan

```python
WEEK_3_4 = {
    "discovery_agent": [
        "3-pass discovery flow",
        "DDD modeling with Sonnet",
        "Interactive questions UI",
        "Validation of discovery output"
    ],

    "masterplan_generation": [
        "Monolithic generation (64K output)",
        "RAG retrieval integration",
        "Dependency graph creation",
        "MasterPlan validation"
    ],

    "deliverable": "Discovery ‚Üí MasterPlan pipeline working"
}
```

### Week 5-6: Task Execution

```python
WEEK_5_6 = {
    "execution_engine": [
        "Sequential task executor",
        "Context building con caching",
        "Scratchpad management",
        "File creation & workspace updates"
    ],

    "validation": [
        "JSON validation",
        "Syntax checking (Python/JS)",
        "File existence validation",
        "Retry logic"
    ],

    "deliverable": "End-to-end execution working"
}
```

### Week 7-8: Polish & Testing

```python
WEEK_7_8 = {
    "observability": [
        "Prometheus metrics (cost, latency, success)",
        "Logging (structured JSON logs)",
        "Progress streaming to frontend",
        "Error reporting"
    ],

    "testing": [
        "Integration tests (full pipeline)",
        "Cost tracking validation",
        "Cache hit rate monitoring",
        "Manual QA on 10+ projects"
    ],

    "deliverable": "MVP ready for beta users"
}
```

---

## üìä COMPARATIVA FINAL: Estrategias MVP

| Strategy | Cost | Latency | Success | Complexity | Recommendation |
|----------|------|---------|---------|------------|----------------|
| **All-Haiku** | $7.94 | 12-15min | 65-70% | Low | ‚ö†Ô∏è Budget pero quality baja |
| **Hybrid (60H/40S)** | $11.88 | 15-18min | 75-80% | Medium | ‚úÖ **RECOMENDADO** |
| **All-Sonnet** | $14.21 | 15-18min | 80-85% | Medium | ‚úÖ Conservative |
| **With Opus** | $18-22 | 18-22min | 85-90% | High | ‚ùå Over-engineered para MVP |

---

## ‚úÖ CONCLUSIONES FINALES (MVP)

### Arquitectura Viable

```
‚úÖ TOTALMENTE VIABLE con Claude 4.x family
‚úÖ PROMPT CACHING es el game-changer (32% cost reduction)
‚úÖ SEQUENTIAL execution suficiente para MVP
‚úÖ 64K OUTPUT permite MasterPlan monol√≠tico
```

### Costos Realistas

```python
MVP_REALISTIC_COST = {
    "hybrid_strategy": "$11.88 per project",
    "conservative_all_sonnet": "$14.21 per project",
    "budget_all_haiku": "$7.94 per project",

    "recommendation": "Hybrid (60% Haiku, 40% Sonnet)",
    "rationale": "Best cost/quality balance"
}
```

### Latency Alcanzable

```python
MVP_LATENCY = {
    "tier_2_parallel_2x": "15-18 minutos",
    "acceptable": True,
    "improvement_v2": "Parallel 5x ‚Üí 7-10 minutos"
}
```

### Success Rate Honesto

```python
MVP_SUCCESS = {
    "v1_expected": "70-75%",
    "v1_optimistic": "75-80%",
    "v2_target": "85-90%",

    "definition": "Code compiles + runs + basic functionality works"
}
```

### Key Decisions

1. **Prompt Caching**: Must-have desde d√≠a 1 (32% savings)
2. **Model Strategy**: Hybrid 60% Haiku / 40% Sonnet
3. **Execution**: Sequential para MVP (parallel en v2)
4. **Validation**: JSON + syntax + basic checks
5. **Retries**: 1 retry autom√°tico, max 2 attempts

### Scope MVP

```python
MVP_SCOPE = {
    "in_scope": [
        "Discovery (3 passes)",
        "MasterPlan generation (monolithic)",
        "Task execution (sequential)",
        "Prompt caching",
        "Basic validation",
        "Retry logic",
        "Progress streaming",
        "Cost tracking"
    ],

    "out_of_scope_v1": [
        "Parallel execution",
        "Opus model",
        "Extended context (1M)",
        "Advanced testing",
        "Security scanning",
        "Workspace integration (existing code)",
        "Multi-language support"
    ]
}
```

---

**Estado**: ‚úÖ ARQUITECTURA MVP DEFINIDA
**Viabilidad**: ‚úÖ COMPROBADA con n√∫meros realistas
**Costo target**: $11.88 per 50-task project (Hybrid)
**Latency target**: 15-18 minutos (Tier 2)
**Success target**: 75-80% (primera vez)
**Timeline**: 8 semanas para MVP funcional

**Prompt caching cambia el juego - 32% cost reduction!** üöÄ
