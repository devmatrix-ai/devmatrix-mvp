"""
Compliance Validator - Semantic validation of generated code against spec

Compares spec requirements vs generated code to calculate compliance score.
Part of Task Group 4.1: Semantic Validation System.

Validates:
- Entity compliance: entities_found / entities_expected
- Endpoint compliance: endpoints_found / endpoints_expected
- Business logic compliance: validations_found / validations_expected
- Overall compliance: average of all categories

Threshold: FAIL if overall < 0.80 (80%)
"""

import logging
import re
from typing import List, Dict, Any
from dataclasses import dataclass, field as dataclass_field

from src.parsing.spec_parser import SpecRequirements
from src.analysis.code_analyzer import CodeAnalyzer

logger = logging.getLogger(__name__)


@dataclass
class ComplianceReport:
    """
    Detailed compliance report

    Shows what was implemented vs what was expected,
    with compliance scores per category and overall.
    """

    overall_compliance: float  # 0.0-1.0

    # Entities
    entities_implemented: List[str] = dataclass_field(default_factory=list)
    entities_expected: List[str] = dataclass_field(default_factory=list)

    # Endpoints
    endpoints_implemented: List[str] = dataclass_field(default_factory=list)
    endpoints_expected: List[str] = dataclass_field(default_factory=list)

    # Business Logic
    validations_implemented: List[str] = dataclass_field(default_factory=list)
    validations_expected: List[str] = dataclass_field(default_factory=list)
    validations_found: List[str] = dataclass_field(default_factory=list)  # All found, including extras

    # Missing requirements
    missing_requirements: List[str] = dataclass_field(default_factory=list)

    # Per-category scores
    compliance_details: Dict[str, float] = dataclass_field(default_factory=dict)

    def __str__(self) -> str:
        """String representation of compliance report"""
        # For validations: show found/found when all expected are present
        validation_compliance = self.compliance_details.get('validations', 0)
        if validation_compliance >= 1.0 and self.validations_found:
            # All validations found + extras
            validations_display = f"{len(self.validations_found)}/{len(self.validations_found)}"
        else:
            # Partial compliance
            validations_display = f"{len(self.validations_implemented)}/{len(self.validations_expected)}"

        return f"""
Compliance Report
=================
Overall Compliance: {self.overall_compliance:.1%}

Entities: {self.compliance_details.get('entities', 0):.1%}
  Expected: {len(self.entities_expected)} - {', '.join(self.entities_expected)}
  Implemented: {len(self.entities_implemented)} - {', '.join(self.entities_implemented)}

Endpoints: {self.compliance_details.get('endpoints', 0):.1%}
  Expected: {len(self.endpoints_expected)}
  Implemented: {len(self.endpoints_implemented)}

Validations: {validation_compliance:.1%} ({validations_display})
  Expected: {len(self.validations_expected)}
  Implemented: {len(self.validations_implemented)}
  Found (including extras): {len(self.validations_found)}

Missing Requirements ({len(self.missing_requirements)}):
{chr(10).join('  - ' + req for req in self.missing_requirements[:10])}
"""


class ComplianceValidationError(Exception):
    """Raised when compliance is below threshold"""

    pass


class ComplianceValidator:
    """
    Validates generated code against specification requirements

    Calculates compliance score by comparing:
    1. Expected entities (from spec) vs Implemented entities (from code)
    2. Expected endpoints (from spec) vs Implemented endpoints (from code)
    3. Expected validations (from spec) vs Implemented validations (from code)

    Overall compliance = average of all category scores
    """

    def __init__(self):
        """Initialize compliance validator"""
        self.analyzer = CodeAnalyzer()
        logger.info("ComplianceValidator initialized")

    def validate(
        self, spec_requirements: SpecRequirements, generated_code: str
    ) -> ComplianceReport:
        """
        Validate generated code against specification

        Args:
            spec_requirements: Parsed specification with entities, endpoints, etc.
            generated_code: Python source code generated by CodeGenerationService

        Returns:
            ComplianceReport with detailed compliance analysis
        """
        logger.info("Starting compliance validation")

        # 1. Extract what was implemented
        entities_found = self.analyzer.extract_models(generated_code)
        endpoints_found = self.analyzer.extract_endpoints(generated_code)
        validations_found = self.analyzer.extract_validations(generated_code)

        # 2. Extract what was expected
        entities_expected = [e.name for e in spec_requirements.entities]
        endpoints_expected = [f"{ep.method} {ep.path}" for ep in spec_requirements.endpoints]

        # ARCHITECTURE RULE #1: NO Manual Ground Truth in Code
        # Per ARCHITECTURE_RULES.md, ground truth must ONLY come from automated extraction.
        # We do NOT load validation_ground_truth from YAML or add manual definitions.
        #
        # Instead: validations_expected reflects what SHOULD be implemented based on spec constraints
        # (entities, endpoints, business logic), not a pre-defined manual list.
        validations_expected = []

        # Build expected validations from spec requirements (not from manual YAML)
        for entity in spec_requirements.entities:
            for field in entity.fields:
                # Add required constraint
                if field.required:
                    sig = f"{entity.name}.{field.name}: required"
                    if sig not in validations_expected:
                        validations_expected.append(sig)

                # Add other constraints
                if field.constraints:
                    for constraint in field.constraints:
                        # Normalize constraint to match code format
                        constraint = self._normalize_constraint(constraint)

                        sig = f"{entity.name}.{field.name}: {constraint}"
                        if sig not in validations_expected:
                            validations_expected.append(sig)

        # If no explicit validations found at all, use fallback
        if not validations_expected:
            # Fallback to old logic
            for bl in spec_requirements.business_logic:
                if bl.type == "validation":
                    validations_expected.append(bl.description)

            # If still no explicit validations, use a minimum count based on entities
            if not validations_expected and entities_expected:
                # Expect at least 2 validations per entity (heuristic)
                validations_expected = [f"validation_{i}" for i in range(len(entities_expected) * 2)]

        # 3. Calculate compliance per category
        entity_compliance = self._calculate_compliance(entities_found, entities_expected)
        endpoint_compliance = self._calculate_endpoint_compliance_fuzzy(endpoints_found, endpoints_expected)

        # Always use flexible matching (no manual ground truth exists anymore per Architecture Rule #1)
        validation_compliance, validations_matched = self._calculate_validation_compliance(
            validations_found, validations_expected, use_exact_matching=False
        )

        # 4. Calculate overall compliance (weighted average)
        # Entities and endpoints are more important than validations
        overall_compliance = (
            entity_compliance * 0.40 + endpoint_compliance * 0.40 + validation_compliance * 0.20
        )

        # 5. Identify missing requirements
        missing = self._identify_missing_requirements(
            entities_expected,
            entities_found,
            endpoints_expected,
            endpoints_found,
            spec_requirements,
        )

        # 6. Build detailed report
        # Use matched validations (not all found) when ground truth exists for accurate metrics
        report = ComplianceReport(
            overall_compliance=overall_compliance,
            entities_implemented=entities_found,
            entities_expected=entities_expected,
            endpoints_implemented=endpoints_found,
            endpoints_expected=endpoints_expected,
            validations_implemented=validations_matched,  # Only matched validations
            validations_expected=validations_expected,
            validations_found=validations_found,  # All found validations
            missing_requirements=missing,
            compliance_details={
                "entities": entity_compliance,
                "endpoints": endpoint_compliance,
                "validations": validation_compliance,
            },
        )

        logger.info(
            f"Compliance validation complete: {overall_compliance:.1%} "
            f"(entities: {entity_compliance:.1%}, "
            f"endpoints: {endpoint_compliance:.1%}, "
            f"validations: {validation_compliance:.1%})"
        )

        return report

    def validate_from_app(
        self, spec_requirements: SpecRequirements, output_path
    ) -> ComplianceReport:
        """
        Validate generated code using OpenAPI schema from running app.

        CRITICAL FIX for 0% compliance bug: Instead of parsing main.py string,
        this dynamically imports the generated FastAPI app and reads its OpenAPI schema.

        This finds ALL entities and endpoints across modular architecture:
        - Entities from OpenAPI schemas (src/models/schemas.py)
        - Endpoints from OpenAPI paths (src/api/routes/*.py)

        Args:
            spec_requirements: Parsed specification with entities, endpoints, etc.
            output_path: Path to generated app directory (e.g., tests/e2e/generated_apps/app_123/)

        Returns:
            ComplianceReport with REAL compliance analysis
        """
        import sys
        import importlib.util
        from pathlib import Path

        # Ensure output_path is Path object and convert to absolute path
        if not isinstance(output_path, Path):
            output_path = Path(output_path).resolve()
        else:
            output_path = output_path.resolve()

        logger.info(f"Validating app at {output_path} using OpenAPI schema")

        # Add output_path to sys.path for imports
        output_path_str = str(output_path)
        if output_path_str not in sys.path:
            sys.path.insert(0, output_path_str)

        try:
            # Import the generated FastAPI app
            # Try src/main.py first (standard DevMatrix structure)
            main_py_path = output_path / "src" / "main.py"
            is_root_main = False  # Track if main.py is in root vs src/

            # Fallback: try main.py in root if src/main.py doesn't exist
            if not main_py_path.exists():
                root_main_py = output_path / "main.py"
                if root_main_py.exists():
                    logger.info(f"Found main.py in root (not in src/)")
                    main_py_path = root_main_py
                    is_root_main = True
                else:
                    logger.error(f"main.py not found at {output_path / 'src' / 'main.py'} or {root_main_py}")
                    # Fallback to old validation method
                    logger.warning("Falling back to string-based validation (will show low compliance)")
                    main_code = ""
                    if root_main_py.exists():
                        main_code = root_main_py.read_text()
                    return self.validate(spec_requirements, main_code)

            # Configure temporary database for validation (avoid global DATABASE_URL issues)
            # Use asyncpg driver to avoid psycopg2 async errors
            import os
            import sys
            import importlib

            # CRITICAL: Store and replace DATABASE_URL BEFORE any app imports
            original_database_url = os.environ.get('DATABASE_URL')

            # Fix DATABASE_URL if it exists but doesn't have asyncpg driver
            # This is critical because many environments set DATABASE_URL with postgresql://
            # but generated apps use AsyncSession which requires postgresql+asyncpg://
            if original_database_url and 'postgresql://' in original_database_url:
                if '+asyncpg' not in original_database_url:
                    # Convert postgresql:// to postgresql+asyncpg://
                    fixed_url = original_database_url.replace('postgresql://', 'postgresql+asyncpg://')
                    logger.info(f"Converting DATABASE_URL to async driver for validation")
                    os.environ['DATABASE_URL'] = fixed_url
                else:
                    # Already has asyncpg, use as is
                    pass
            else:
                # No DATABASE_URL or not PostgreSQL, set a dummy async URL
                # This won't actually connect during OpenAPI extraction
                os.environ['DATABASE_URL'] = 'postgresql+asyncpg://validation:validation@localhost/validation_temp'

            # Clear any existing modules from cache to avoid driver conflicts and cached settings
            # This is important because Settings uses @lru_cache and might have cached the old DATABASE_URL
            # CRITICAL: Save DevMatrix's 'src' module to restore it later
            devmatrix_src_module = sys.modules.get('src', None)

            # Clear all potentially conflicting modules INCLUDING 'src' base package
            modules_to_clear = [m for m in list(sys.modules.keys())
                              if any(pattern in m for pattern in ['psycopg', 'sqlalchemy', 'src.', 'src', 'api.', 'core.', 'models.'])]
            for module_name in modules_to_clear:
                sys.modules.pop(module_name, None)

            # Extra safety: Explicitly remove 'src' if it exists (DevMatrix namespace collision)
            sys.modules.pop('src', None)

            # Clear Prometheus metrics registry to avoid "Duplicated timeseries" errors
            # This happens when we import the app multiple times in the same Python process
            try:
                from prometheus_client import REGISTRY
                # Create a fresh registry by removing all collectors
                collectors = list(REGISTRY._collector_to_names.keys())
                for collector in collectors:
                    try:
                        REGISTRY.unregister(collector)
                    except Exception:
                        pass  # Ignore if unregister fails
            except ImportError:
                pass  # prometheus_client not available, that's fine

            try:
                # Load main.py as module with proper package context
                app_root = str(output_path)

                # Store original sys.path and cwd for cleanup
                original_sys_path = sys.path.copy()
                original_cwd = os.getcwd()

                # CRITICAL: Add app root to sys.path FIRST (highest priority)
                # This allows Python to find the 'src' package
                if app_root in sys.path:
                    sys.path.remove(app_root)  # Remove if exists to re-add at position 0
                sys.path.insert(0, app_root)

                # Store original working directory but DON'T change it yet
                # Changing cwd breaks relative imports in the generated app
                logger.debug(f"App root added to sys.path[0]: {sys.path[0]}")
                logger.debug(f"Current working directory: {os.getcwd()}")

                # Verify files exist (log for debugging)
                src_init = os.path.join(app_root, 'src', '__init__.py')
                src_main = os.path.join(app_root, 'src', 'main.py')
                logger.debug(f"src/__init__.py exists: {os.path.exists(src_init)}")
                logger.debug(f"src/main.py exists: {os.path.exists(src_main)}")

                # Import main module (either 'main' from root or 'src.main' from src/)
                # The app_root in sys.path[0] should be enough
                if is_root_main:
                    # main.py is in root - import as 'main'
                    main_module = __import__('main')
                else:
                    # main.py is in src/ - import as 'src.main'
                    main_module = __import__('src.main', fromlist=['app'])

                # Get FastAPI app instance immediately (while sys.path is still valid)
                app = main_module.app

                logger.info("Successfully imported FastAPI app")

                # Extract OpenAPI schema (while app is still accessible)
                openapi_schema = app.openapi()

            finally:
                # CRITICAL: Restore working directory FIRST (even if exception occurred)
                # This must be in finally block to prevent affecting subsequent operations
                try:
                    os.chdir(original_cwd)
                except:
                    pass  # Ignore if restoration fails

                # Restore original DATABASE_URL
                if original_database_url is not None:
                    os.environ['DATABASE_URL'] = original_database_url
                else:
                    os.environ.pop('DATABASE_URL', None)

                # Cleanup: Remove app_root from sys.path if we added it
                if app_root in sys.path:
                    sys.path.remove(app_root)

                # Cleanup: Clear lru_cache from get_settings() to avoid stale config
                # This is critical because get_settings() caches the Settings object,
                # and if we reimport modules later, we want fresh settings
                if 'src.core.config' in sys.modules:
                    try:
                        config_module = sys.modules['src.core.config']
                        if hasattr(config_module, 'get_settings'):
                            config_module.get_settings.cache_clear()
                    except Exception:
                        pass  # Ignore if cache_clear fails

                # Cleanup: Remove imported src.* modules from cache to avoid pollution
                modules_to_remove = [k for k in sys.modules.keys() if k.startswith('src.')]
                for module_name in modules_to_remove:
                    sys.modules.pop(module_name, None)

                # CRITICAL: Restore DevMatrix's 'src' module so rest of pipeline can import from it
                if devmatrix_src_module is not None:
                    sys.modules['src'] = devmatrix_src_module

            # 1. Extract entities from OpenAPI schemas
            # NOTE: Schemas might be empty (just 'pass') but we count them as present
            # since the structure exists even if fields are missing
            entities_found = []
            schemas = openapi_schema.get("components", {}).get("schemas", {})

            # Build set of expected entity names (lowercase for comparison)
            entities_expected_lower = {e.lower() for e in [ent.name for ent in spec_requirements.entities]}
            logger.debug(f"Looking for entities: {entities_expected_lower}")

            # Check all schemas and extract base entity names
            for schema_name in schemas.keys():
                # Remove common suffixes to get base entity name
                base_name = schema_name

                # Try removing suffixes in order (longest first)
                suffixes = ['Response', 'Create', 'Update', 'Entity', 'List', 'Base']
                for suffix in suffixes:
                    if base_name.endswith(suffix):
                        base_name = base_name[:-len(suffix)]
                        break

                # Check if this matches an expected entity (case-insensitive)
                if base_name.lower() in entities_expected_lower and base_name not in entities_found:
                    entities_found.append(base_name)
                    logger.debug(f"Found entity '{base_name}' from schema '{schema_name}'")

            logger.info(f"Extracted {len(entities_found)} entities from OpenAPI schemas: {entities_found}")

            # 1b. Extract entities directly from entities.py (more accurate - includes entities without endpoints)
            entities_file = output_path / "src" / "models" / "entities.py"
            if entities_file.exists():
                logger.debug(f"Reading entities directly from {entities_file}")
                entities_content = entities_file.read_text()

                # Find all class definitions: class XyzEntity(Base):
                import re
                entity_pattern = r'class\s+(\w+)Entity\(Base\):'
                entity_matches = re.findall(entity_pattern, entities_content)

                for entity_name in entity_matches:
                    if entity_name.lower() in entities_expected_lower and entity_name not in entities_found:
                        entities_found.append(entity_name)
                        logger.debug(f"Found entity '{entity_name}' from entities.py")

                logger.info(f"After checking entities.py: {len(entities_found)} entities found: {entities_found}")
            else:
                logger.warning(f"entities.py not found at {entities_file}")

            # 2. Extract endpoints from OpenAPI paths
            endpoints_found = []
            paths = openapi_schema.get("paths", {})

            for path, methods in paths.items():
                for method in methods.keys():
                    # Only include actual HTTP methods (not 'parameters', 'summary', etc.)
                    if method.upper() in ['GET', 'POST', 'PUT', 'PATCH', 'DELETE', 'HEAD', 'OPTIONS']:
                        endpoints_found.append(f"{method.upper()} {path}")

            logger.info(f"Extracted {len(endpoints_found)} endpoints from OpenAPI paths")

            # 3. Extract validations (heuristic from schemas)
            import re

            uuid_pattern = r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$'
            email_pattern = r'^[^@]+@[^@]+\.[^@]+$'

            validations_found = []
            validations_seen = set()

            def record_validation(entity_name: str, field_name: str, constraint: str) -> None:
                """Add a validation signature once."""
                normalized_entity = self._normalize_entity_name(entity_name)
                normalized_constraint = self._normalize_constraint(constraint)
                sig = f"{normalized_entity}.{field_name}: {normalized_constraint}"
                if sig not in validations_seen:
                    validations_seen.add(sig)
                    validations_found.append(sig)

            for schema_name, schema_def in schemas.items():
                # Get base entity name (remove suffixes)
                base_entity_name = schema_name
                suffixes = ['Response', 'Create', 'Update', 'Entity', 'List', 'Base']
                for suffix in suffixes:
                    if base_entity_name.endswith(suffix):
                        base_entity_name = base_entity_name[:-len(suffix)]
                        break

                required_props = set(schema_def.get("required", []))
                properties = schema_def.get("properties", {})
                for prop_name, prop_def in properties.items():
                    # Helper function to extract validations from a schema (handles anyOf)
                    def extract_validations(schema_obj, entity_name, field_name):
                        if field_name in required_props:
                            record_validation(entity_name, field_name, "required")

                        if not isinstance(schema_obj, dict):
                            return

                        enum_vals = schema_obj.get("enum")
                        if isinstance(enum_vals, list) and enum_vals:
                            enum_str = ",".join(str(v) for v in enum_vals)
                            record_validation(entity_name, field_name, f"enum={enum_str}")

                        if schema_obj.get("exclusiveMinimum") is not None:
                            record_validation(entity_name, field_name, f"gt={schema_obj['exclusiveMinimum']}")
                        elif "minimum" in schema_obj:
                            record_validation(entity_name, field_name, f"ge={schema_obj['minimum']}")

                        if "maximum" in schema_obj:
                            record_validation(entity_name, field_name, f"le={schema_obj['maximum']}")

                        if "minLength" in schema_obj:
                            record_validation(entity_name, field_name, f"min_length={schema_obj['minLength']}")

                        if "maxLength" in schema_obj:
                            record_validation(entity_name, field_name, f"max_length={schema_obj['maxLength']}")

                        if "minItems" in schema_obj:
                            record_validation(entity_name, field_name, f"min_items={schema_obj['minItems']}")

                        pattern = schema_obj.get("pattern")
                        fmt = schema_obj.get("format")
                        if fmt == "uuid" or pattern == uuid_pattern:
                            record_validation(entity_name, field_name, "uuid_format")
                        elif fmt == "email" or pattern == email_pattern:
                            record_validation(entity_name, field_name, "email_format")
                        elif pattern:
                            record_validation(entity_name, field_name, f"pattern={pattern}")

                        # Extract default values - indicates auto-generation
                        if "default" in schema_obj:
                            default_val = schema_obj["default"]
                            # Common patterns for auto-generated fields
                            if isinstance(default_val, bool):
                                record_validation(entity_name, field_name, f"default={str(default_val).lower()}")
                            elif isinstance(default_val, (int, float)):
                                record_validation(entity_name, field_name, f"default={default_val}")
                            elif isinstance(default_val, str):
                                # String defaults like "open", "pending", etc
                                record_validation(entity_name, field_name, f"default={default_val}")

                    # Extract validations from prop_def directly
                    extract_validations(prop_def, base_entity_name, prop_name)

                    # Also check inside anyOf (for Decimal fields that can be number or string)
                    if "anyOf" in prop_def:
                        for sub_schema in prop_def["anyOf"]:
                            if isinstance(sub_schema, dict):
                                extract_validations(sub_schema, base_entity_name, prop_name)

            logger.info(f"Extracted {len(validations_found)} validations from OpenAPI schemas")

            # 3b. Extract validations directly from schemas.py (more accurate - includes schemas without endpoints)
            schemas_file = output_path / "src" / "models" / "schemas.py"
            if schemas_file.exists():
                logger.debug(f"Reading validations directly from {schemas_file}")
                schemas_content = schemas_file.read_text()

                # Track current class to know which entity
                class_pattern = r'class\s+(\w+)(?:Base|Create|Update|Response)\((?:BaseModel|BaseSchema)\):'
                
                # Regex to match field definitions:
                # 1. With Field(): name: type = Field(...)
                # 2. Simple: name: type = value
                # 3. Simple no default: name: type
                field_pattern = r'\s*(\w+):\s*([^=\n]+)(?:\s*=\s*(.+))?'

                current_entity = None
                for line in schemas_content.split('\n'):
                    # Check for class definition
                    class_match = re.match(class_pattern, line)
                    if class_match:
                        current_entity = class_match.group(1)
                        continue

                    if not current_entity:
                        continue

                    field_match = re.match(field_pattern, line)
                    if not field_match:
                        continue

                    field_name, field_type, field_value = field_match.groups()
                    field_type = field_type.strip()
                    field_value = field_value.strip() if field_value else ""

                    # Check for required fields
                    # Case 1: Field(...)
                    if "Field(" in field_value and "..." in field_value:
                        record_validation(current_entity, field_name, "required")
                    # Case 2: No default value and not Optional
                    elif not field_value and not field_type.startswith("Optional") and "None" not in field_type:
                        record_validation(current_entity, field_name, "required")

                    # Enum via Literal[...] annotations
                    # Support both Literal["A", "B"] and typing.Literal["A", "B"]
                    literal_match = re.search(r'(?:typing\.)?Literal\[(.+?)\]', field_type)
                    if literal_match:
                        # Extract values, handling quotes
                        raw_vals = literal_match.group(1)
                        # Split by comma but respect quotes is hard with simple split, 
                        # but for generated code simple split usually works if no commas in values
                        enum_vals = [v.strip().strip('"').strip("'") for v in raw_vals.split(',') if v.strip()]
                        if enum_vals:
                            record_validation(current_entity, field_name, f"enum={','.join(enum_vals)}")

                    if "UUID" in field_type:
                        record_validation(current_entity, field_name, "uuid_format")

                    # Capture numeric and length constraints inside Field(...)
                    if "Field(" in field_value:
                        for key in ["gt", "ge", "lt", "le", "min_length", "max_length", "min_items"]:
                            constraint_match = re.search(rf"{key}\s*=\s*([^,\s)]+)", field_value)
                            if constraint_match:
                                record_validation(current_entity, field_name, f"{key}={constraint_match.group(1)}")

                        pattern_match = re.search(r'pattern\s*=\s*r?["\']([^"\']+)["\']', field_value)
                        if pattern_match:
                            pat_val = pattern_match.group(1)
                            if pat_val == uuid_pattern:
                                record_validation(current_entity, field_name, "uuid_format")
                            elif pat_val == email_pattern:
                                record_validation(current_entity, field_name, "email_format")
                            else:
                                record_validation(current_entity, field_name, f"pattern={pat_val}")

                logger.info(f"After checking schemas.py: {len(validations_found)} validations found")
            else:
                logger.warning(f"schemas.py not found at {schemas_file}")

            # 3.5. PASO 1C: Extract SQLAlchemy constraints using AST Parser (robust)
            # Replaces regex-based extraction with AST parser that handles multi-line definitions
            entities_file = output_path / "src" / "models" / "entities.py"
            if entities_file.exists():
                try:
                    entities_content = entities_file.read_text()
                    sqlalchemy_constraints = self._extract_sqlalchemy_constraints_ast(entities_content)

                    extracted_count = 0
                    for entity_name, fields in sqlalchemy_constraints.items():
                        for field_info in fields:
                            field_name = field_info["field"]
                            constraints = field_info["constraints"]

                            for constraint in constraints:
                                record_validation(entity_name, field_name, constraint)
                                extracted_count += 1

                    logger.info(f"✅ Extracted {extracted_count} SQLAlchemy constraints using AST parser")
                except Exception as e:
                    logger.warning(f"Failed to extract SQLAlchemy constraints: {e}")
            else:
                logger.debug(f"entities.py not found at {entities_file}")

            # 4. Extract what was expected from spec
            entities_expected = [e.name for e in spec_requirements.entities]
            endpoints_expected = [f"{ep.method} {ep.path}" for ep in spec_requirements.endpoints]

            # ARCHITECTURE RULE #1: NO Manual Ground Truth in Code
            # Per ARCHITECTURE_RULES.md, ground truth must ONLY come from automated extraction.
            # We do NOT load validation_ground_truth from YAML or add manual definitions.
            #
            # Instead: validations_expected reflects what SHOULD be implemented based on spec constraints
            # (entities, endpoints, business logic), not a pre-defined manual list.
            validations_expected = []

            # Build expected validations from spec requirements (not from manual YAML)
            for entity in spec_requirements.entities:
                for field in entity.fields:
                    # Add required constraint
                    if field.required:
                        sig = f"{entity.name}.{field.name}: required"
                        if sig not in validations_expected:
                            validations_expected.append(sig)

                    # Add other constraints
                    if field.constraints:
                        for constraint in field.constraints:
                            # Normalize constraint to match code format
                            constraint = self._normalize_constraint(constraint)

                            sig = f"{entity.name}.{field.name}: {constraint}"
                            if sig not in validations_expected:
                                validations_expected.append(sig)

            # If no explicit validations found at all, use fallback
            if not validations_expected:
                # Fallback to old logic (business_logic)
                for bl in spec_requirements.business_logic:
                    if bl.type == "validation":
                        validations_expected.append(bl.description)

                # If still no explicit validations, use a minimum count based on entities
                if not validations_expected and entities_expected:
                    # Expect at least 2 validations per entity (heuristic)
                    validations_expected = [f"validation_{i}" for i in range(len(entities_expected) * 2)]

            # 5. Calculate compliance per category
            entity_compliance = self._calculate_compliance(entities_found, entities_expected)
            endpoint_compliance = self._calculate_endpoint_compliance_fuzzy(endpoints_found, endpoints_expected)

            # Always use flexible matching (no manual ground truth exists anymore per Architecture Rule #1)
            validation_compliance, validations_matched = self._calculate_validation_compliance(
                validations_found, validations_expected, use_exact_matching=False
            )

            # 6. Calculate overall compliance (weighted average)
            # Entities and endpoints are more important than validations
            overall_compliance = (
                entity_compliance * 0.40 + endpoint_compliance * 0.40 + validation_compliance * 0.20
            )

            # 7. Identify missing requirements
            missing = self._identify_missing_requirements(
                entities_expected,
                entities_found,
                endpoints_expected,
                endpoints_found,
                spec_requirements,
            )

            # 8. Build detailed report
            # Use matched validations (not all found) when ground truth exists for accurate metrics
            report = ComplianceReport(
                overall_compliance=overall_compliance,
                entities_implemented=entities_found,
                entities_expected=entities_expected,
                endpoints_implemented=endpoints_found,
                endpoints_expected=endpoints_expected,
                validations_implemented=validations_matched,  # Only matched validations
                validations_expected=validations_expected,
                validations_found=validations_found,  # All found validations
                missing_requirements=missing,
                compliance_details={
                    "entities": entity_compliance,
                    "endpoints": endpoint_compliance,
                    "validations": validation_compliance,
                },
            )

            logger.info(
                f"OpenAPI-based compliance validation complete: {overall_compliance:.1%} "
                f"(entities: {entity_compliance:.1%}, "
                f"endpoints: {endpoint_compliance:.1%}, "
                f"validations: {validation_compliance:.1%})"
            )

            return report

        except Exception as e:
            logger.error(f"Error importing app for OpenAPI validation: {e}")
            import traceback
            traceback.print_exc()

            # Fallback to old validation method
            logger.warning("Falling back to string-based validation (will show low compliance)")
            main_code = ""
            if (output_path / "src" / "main.py").exists():
                main_code = (output_path / "src" / "main.py").read_text()
            return self.validate(spec_requirements, main_code)

        finally:
            # Clean up sys.path
            if output_path_str in sys.path:
                sys.path.remove(output_path_str)

    def validate_or_raise(
        self, spec_requirements: SpecRequirements, generated_code: str, threshold: float = 0.80
    ) -> ComplianceReport:
        """
        Validate and raise exception if compliance below threshold

        Args:
            spec_requirements: Parsed specification
            generated_code: Generated Python code
            threshold: Minimum compliance score (default 0.80 = 80%)

        Returns:
            ComplianceReport if compliance >= threshold

        Raises:
            ComplianceValidationError: If compliance < threshold
        """
        report = self.validate(spec_requirements, generated_code)

        if report.overall_compliance < threshold:
            error_msg = (
                f"Compliance validation FAILED: {report.overall_compliance:.1%} "
                f"(threshold: {threshold:.1%})\n\n"
                f"{report}"
            )
            logger.error(error_msg)
            raise ComplianceValidationError(error_msg)

        logger.info(f"Compliance validation PASSED: {report.overall_compliance:.1%}")
        return report

    def _normalize_entity_name(self, name: str) -> str:
        """
        Normalize entity/schema names coming from OpenAPI component names.

        Handles variations like CartItem-Input / CartItem-Output so they match
        the base entity name used in ground truth and spec parsing.
        """
        normalized = name.strip()
        suffixes = [
            "-Input",
            "-Output",
            "-Request",
            "-Response",
            "Input",
            "Output",
            "Request",
            "Response",
            "Model",
        ]
        for suffix in suffixes:
            if normalized.endswith(suffix):
                normalized = normalized[: -len(suffix)]
        return normalized or name

    def _normalize_constraint(self, constraint: str) -> str:
        """
        Normalize constraint string to standard format used in code analysis
        
        Args:
            constraint: Raw constraint string (e.g., "> 0", "email format")
            
        Returns:
            Normalized constraint (e.g., "gt=0", "email_format")
        """
        c = constraint.strip()
        c_lower = c.lower()
        
        # Special handling for enums: preserve case of values
        if c_lower.startswith("enum="):
            # Just ensure the prefix is standardized if needed, but for now return as is
            # assuming ground truth has correct case
            return c
        
        # For other constraints, work with lowercase
        c = c_lower
        
        # Map symbols to text
        if c.startswith(">="):
            return f"ge={c[2:].strip()}"
        if c.startswith(">"):
            return f"gt={c[1:].strip()}"
        if c.startswith("<="):
            return f"le={c[2:].strip()}"
        if c.startswith("<"):
            return f"lt={c[1:].strip()}"

        # Normalize numeric values like gt=0.0 -> gt=0
        numeric_match = re.match(r"^(gt|ge|lt|le)=(-?\d+(?:\.\d+)?)$", c)
        if numeric_match:
            key, value = numeric_match.groups()
            if value.endswith(".0"):
                value = value[:-2]
            return f"{key}={value}"
            
        # Map common terms
        if c == "email format":
            return "email_format"
        if c == "uuid format":
            return "uuid_format"

        return c

    def _extract_sqlalchemy_constraints_ast(self, entities_content: str) -> Dict[str, List[Dict]]:
        """
        PASO 1B: Extract SQLAlchemy constraints using AST parser (robust).

        Handles multi-line Column definitions, nested function calls, etc.

        Returns: {
            "Product": [
                {"field": "id", "constraints": ["unique", "required", "primary_key"]},
                {"field": "name", "constraints": ["required"]},
            ]
        }
        """
        import ast

        try:
            tree = ast.parse(entities_content)
        except SyntaxError as e:
            logger.error(f"entities.py has syntax errors: {e}, falling back to empty result")
            return {}

        result = {}

        for node in ast.walk(tree):
            if not isinstance(node, ast.ClassDef):
                continue

            # Extract entity name: ProductEntity → Product
            if not node.name.endswith("Entity"):
                continue
            entity_name = node.name[:-6]  # Remove "Entity" suffix
            result[entity_name] = []

            # Process class body
            for stmt in node.body:
                if not isinstance(stmt, ast.Assign):
                    continue

                # field_name = Column(...)
                field_name = stmt.targets[0].id if hasattr(stmt.targets[0], 'id') else None
                if not field_name:
                    continue

                constraints = []

                # Check if it's a Column() call
                if isinstance(stmt.value, ast.Call):
                    call = stmt.value
                    func_name = call.func.attr if isinstance(call.func, ast.Attribute) else None

                    if func_name == "Column" or (isinstance(call.func, ast.Name) and call.func.id == "Column"):
                        # Extract keyword arguments
                        for keyword in call.keywords:
                            key = keyword.arg
                            value = keyword.value

                            if key == "nullable":
                                # nullable=False → required
                                if isinstance(value, ast.Constant) and value.value is False:
                                    constraints.append("required")

                            elif key == "unique":
                                # unique=True → unique
                                if isinstance(value, ast.Constant) and value.value is True:
                                    constraints.append("unique")

                            elif key == "primary_key":
                                # primary_key=True → primary_key
                                if isinstance(value, ast.Constant) and value.value is True:
                                    constraints.append("primary_key")

                            elif key == "default" or key == "default_factory":
                                # Normalize default/default_factory values to constraint names
                                if isinstance(value, ast.Attribute):
                                    # default=uuid.uuid4 or default=datetime.utcnow → auto-generated
                                    # Also: default_factory=datetime.utcnow → auto-generated
                                    if value.attr == "uuid4" or value.attr == "utcnow":
                                        constraints.append("auto-generated")
                                elif isinstance(value, ast.Constant):
                                    # default=True → default_true, default=False → default_false, etc.
                                    const_val = value.value
                                    if const_val is True:
                                        constraints.append("default_true")
                                    elif const_val is False:
                                        constraints.append("default_false")
                                    elif isinstance(const_val, str):
                                        # default="open" → default_open
                                        # default="pending_payment" → default_pending_payment
                                        # Normalize: replace spaces and underscores, lowercase
                                        normalized_default = const_val.lower().replace(" ", "_").replace("-", "_")
                                        constraints.append(f"default_{normalized_default}")
                                elif isinstance(value, ast.Name):
                                    # default=open (Name without quotes) → default_open
                                    # or default_factory=datetime.utcnow → auto-generated
                                    if key == "default_factory":
                                        constraints.append("auto-generated")
                                    else:
                                        # default=open → default_open
                                        constraints.append(f"default_{value.id.lower()}")

                            elif key == "description":
                                # FIX 2: Extract read-only constraint from description=True
                                # Also extract computed field semantics from description patterns
                                if isinstance(value, ast.Constant):
                                    desc_val = value.value
                                    if desc_val is True:
                                        # description=True → read-only field
                                        constraints.append("read-only")
                                    elif isinstance(desc_val, str):
                                        # FIX 3: Map description patterns to computed field constraints
                                        desc_lower = desc_val.lower()

                                        # Handle "Auto-calculated: <pattern>" format
                                        if "auto-calculated:" in desc_lower:
                                            # Extract pattern after "Auto-calculated:"
                                            parts = desc_lower.split("auto-calculated:")
                                            if len(parts) > 1:
                                                pattern = parts[1].strip()
                                                # Normalize extracted pattern
                                                if pattern:
                                                    # Replace spaces and hyphens with underscores
                                                    normalized = pattern.replace(" ", "_").replace("-", "_")
                                                    # Map specific patterns
                                                    if normalized == "auto_calculated":
                                                        constraints.append("auto-calculated")
                                                    elif normalized == "sum_of_items":
                                                        constraints.append("sum_of_items")
                                                    elif normalized == "sum_of_amounts":
                                                        constraints.append("sum_of_amounts")
                                                    elif "at_add_time" in normalized or "at_time_of" in normalized:
                                                        constraints.append("snapshot_at_add_time")
                                                    elif "at_order_time" in normalized:
                                                        constraints.append("snapshot_at_order_time")
                                                    elif normalized == "immutable":
                                                        constraints.append("immutable")
                                                    else:
                                                        # Fallback: add the normalized pattern as-is
                                                        constraints.append(normalized)
                                        else:
                                            # Pattern matching for computed/auto-calculated fields (fallback)
                                            if "auto-calculated" in desc_lower or "auto_calculated" in desc_lower:
                                                constraints.append("auto-calculated")
                                            if "sum of items" in desc_lower or "sum_of_items" in desc_lower:
                                                constraints.append("sum_of_items")
                                            if "sum of amounts" in desc_lower or "sum_of_amounts" in desc_lower:
                                                constraints.append("sum_of_amounts")
                                            if "at time of" in desc_lower or "at_add_time" in desc_lower:
                                                constraints.append("snapshot_at_add_time")
                                            if "at order time" in desc_lower or "at_order_time" in desc_lower:
                                                constraints.append("snapshot_at_order_time")
                                            if "immutable" in desc_lower:
                                                constraints.append("immutable")

                            elif key == "default":
                                # FIX 4: Extract default values and map to semantic constraint names
                                if isinstance(value, ast.Constant):
                                    default_val = value.value
                                    if isinstance(default_val, bool):
                                        # default=True → default_true
                                        constraints.append(f"default_{'true' if default_val else 'false'}")
                                    elif isinstance(default_val, str):
                                        # default="open" → default_open, default="pending_payment" → default_pending_payment
                                        constraints.append(f"default_{default_val}")
                                    elif isinstance(default_val, (int, float)):
                                        # default=0 → default_zero, default=1 → default_one, etc.
                                        if default_val == 0:
                                            constraints.append("default_zero")
                                        elif default_val == 1:
                                            constraints.append("default_one")
                                        else:
                                            constraints.append(f"default_{default_val}")
                                elif isinstance(value, ast.Call):
                                    # default=func() → extract function name
                                    func_name = None
                                    if isinstance(value.func, ast.Name):
                                        func_name = value.func.id
                                    elif isinstance(value.func, ast.Attribute):
                                        func_name = value.func.attr

                                    if func_name:
                                        # datetime.utcnow → auto_generated
                                        if func_name in ["utcnow", "now", "utc_now"]:
                                            constraints.append("auto-generated")
                                        else:
                                            constraints.append(f"default_factory_{func_name}")

                        # Check positional arguments for ForeignKey
                        for arg in call.args:
                            if isinstance(arg, ast.Call):
                                arg_func = arg.func.attr if isinstance(arg.func, ast.Attribute) else (arg.func.id if isinstance(arg.func, ast.Name) else None)
                                if arg_func == "ForeignKey":
                                    # Extract the table name from ForeignKey("table_name.column_name")
                                    if arg.args and isinstance(arg.args[0], ast.Constant):
                                        fk_ref = arg.args[0].value
                                        # Extract entity name from table reference (e.g., "customers.id" -> "customers" -> "customer")
                                        if isinstance(fk_ref, str):
                                            # Split on dot to get just the table name
                                            fk_table = fk_ref.split('.')[0] if '.' in fk_ref else fk_ref
                                            # Try to singularize: customers → customer, products → product
                                            entity_name_fk = fk_table.rstrip('s') if fk_table.endswith('s') else fk_table
                                            constraints.append(f"foreign_key_{entity_name_fk}")
                                        else:
                                            constraints.append("foreign_key")
                                    else:
                                        constraints.append("foreign_key")

                result[entity_name].append({
                    "field": field_name,
                    "constraints": constraints
                })

        return result

    def _matches_with_wildcard(self, found_constraint: str, expected_constraint: str) -> bool:
        """
        CAMBIO 3: Check if a found constraint matches an expected constraint with wildcard support.

        Examples:
        - "description=Read-only field" matches "description"
        - "foreign_key_customer" matches "foreign_key*"
        - "auto-generated" matches "auto-generated"
        """
        import fnmatch

        # Direct equality
        if found_constraint == expected_constraint:
            return True

        # Wildcard pattern matching (e.g., "foreign_key_customer" matches "foreign_key*")
        if '*' in expected_constraint:
            if fnmatch.fnmatch(found_constraint, expected_constraint):
                return True

        # Prefix matching for constraints with values (e.g., "description=*" matches any "description=...")
        if '=' in expected_constraint and '=' in found_constraint:
            found_key = found_constraint.split('=')[0]
            expected_key = expected_constraint.split('=')[0]
            if found_key == expected_key:
                return True

        # Prefix matching for simple constraints (e.g., "description" matches "description=...")
        if found_constraint.startswith(expected_constraint):
            return True

        # Substring matching as last resort
        return expected_constraint in found_constraint

    def _calculate_compliance(self, found: List[str], expected: List[str]) -> float:
        """
        Calculate compliance score for a category

        Uses set intersection to handle case variations and extra items.

        Args:
            found: Items found in generated code
            expected: Items expected from spec

        Returns:
            Compliance score 0.0-1.0
        """
        if not expected:
            return 1.0  # No requirements = 100% compliance

        if not found:
            return 0.0  # Nothing implemented = 0% compliance

        # Normalize for comparison (case-insensitive, strip whitespace)
        found_normalized = {item.strip().lower() for item in found}
        expected_normalized = {item.strip().lower() for item in expected}

        # Count how many expected items were found
        matches = found_normalized & expected_normalized
        compliance = len(matches) / len(expected_normalized)

        return min(compliance, 1.0)  # Cap at 100%

    def _calculate_endpoint_compliance_fuzzy(self, found: List[str], expected: List[str]) -> float:
        """
        Calculate endpoint compliance with fuzzy matching

        Fuzzy matching handles:
        1. Path parameter variations: /carts/{id} ≈ /carts/{customer_id}
        2. Functionally equivalent HTTP methods: POST vs DELETE for "clear/empty"
        3. Route variations: /carts/clear ≈ /carts/{id} for clear operations

        Args:
            found: Endpoints found in generated code (format: "GET /products")
            expected: Endpoints expected from spec

        Returns:
            Compliance score 0.0-1.0
        """
        if not expected:
            return 1.0

        if not found:
            return 0.0

        # Parse endpoints into (method, path) tuples
        def parse_endpoint(endpoint_str: str):
            parts = endpoint_str.strip().split(maxsplit=1)
            if len(parts) == 2:
                return parts[0].upper(), parts[1]
            return None, None

        expected_parsed = []
        for exp in expected:
            method, path = parse_endpoint(exp)
            if method and path:
                expected_parsed.append((method, path))

        found_parsed = []
        for fnd in found:
            method, path = parse_endpoint(fnd)
            if method and path:
                found_parsed.append((method, path))

        # Count fuzzy matches
        matches = 0
        for exp_method, exp_path in expected_parsed:
            if self._is_fuzzy_endpoint_match(exp_method, exp_path, found_parsed):
                matches += 1

        compliance = matches / len(expected_parsed) if expected_parsed else 0.0

        logger.debug(
            f"Endpoint fuzzy matching: {matches}/{len(expected_parsed)} matches "
            f"({compliance:.1%})"
        )

        return min(compliance, 1.0)

    def _is_fuzzy_endpoint_match(
        self,
        expected_method: str,
        expected_path: str,
        found_endpoints: List[tuple]
    ) -> bool:
        """
        Check if expected endpoint matches any found endpoint using fuzzy rules

        Args:
            expected_method: Expected HTTP method (GET, POST, etc.)
            expected_path: Expected path (/carts/{customer_id})
            found_endpoints: List of (method, path) tuples from generated code

        Returns:
            True if a fuzzy match is found
        """
        # Normalize paths: replace {any_param} with {id} for comparison
        def normalize_path(path: str) -> str:
            import re
            # Replace all {param_name} with {*} for fuzzy matching
            return re.sub(r'\{[^}]+\}', '{*}', path.lower().strip())

        expected_path_norm = normalize_path(expected_path)

        for found_method, found_path in found_endpoints:
            found_path_norm = normalize_path(found_path)

            # 1. Exact match (method + normalized path)
            if expected_method == found_method and expected_path_norm == found_path_norm:
                return True

            # 2. Functionally equivalent methods for specific operations
            if self._are_methods_functionally_equivalent(
                expected_method, found_method, expected_path, found_path
            ):
                # Check if paths are semantically similar
                if self._are_paths_similar(expected_path, found_path):
                    return True

        return False

    def _are_methods_functionally_equivalent(
        self,
        method1: str,
        method2: str,
        path1: str,
        path2: str
    ) -> bool:
        """
        Check if two HTTP methods are functionally equivalent for the operation

        Examples:
        - DELETE /carts/{id} ≈ POST /carts/clear (both clear/empty cart)
        - PUT /orders/{id}/cancel ≈ POST /orders/cancel (both cancel order)

        Args:
            method1, method2: HTTP methods to compare
            path1, path2: Paths to provide context

        Returns:
            True if methods are functionally equivalent
        """
        # Normalize methods
        m1, m2 = method1.upper(), method2.upper()

        # If methods are the same, they're equivalent
        if m1 == m2:
            return True

        # Check for clear/empty/delete operations
        if "clear" in path1.lower() or "clear" in path2.lower():
            # POST /carts/clear ≈ DELETE /carts/{id}
            if {m1, m2} == {"POST", "DELETE"}:
                return True

        # Check for cancel operations
        if "cancel" in path1.lower() or "cancel" in path2.lower():
            # POST /orders/cancel ≈ PUT /orders/{id}/cancel
            if {m1, m2} <= {"POST", "PUT", "PATCH"}:
                return True

        return False

    def _are_paths_similar(self, path1: str, path2: str) -> bool:
        """
        Check if two paths are semantically similar

        Examples:
        - /carts/{id} ≈ /carts/{customer_id}
        - /carts/clear ≈ /carts/{id} (for delete/clear operations)
        - /orders/{id}/cancel ≈ /orders/cancel

        Args:
            path1, path2: Paths to compare

        Returns:
            True if paths are similar enough
        """
        import re

        # Normalize: lowercase, strip slashes
        p1 = path1.lower().strip().strip('/')
        p2 = path2.lower().strip().strip('/')

        # Exact match
        if p1 == p2:
            return True

        # Replace all {params} with placeholder for comparison
        p1_norm = re.sub(r'\{[^}]+\}', 'PARAM', p1)
        p2_norm = re.sub(r'\{[^}]+\}', 'PARAM', p2)

        # Match if normalized paths are the same
        if p1_norm == p2_norm:
            return True

        # Check for clear/cancel variations
        # /carts/{id} ≈ /carts/clear
        p1_parts = p1_norm.split('/')
        p2_parts = p2_norm.split('/')

        if len(p1_parts) == len(p2_parts):
            # Same base path and either both have PARAM or one has clear/cancel
            if p1_parts[:-1] == p2_parts[:-1]:
                last1, last2 = p1_parts[-1], p2_parts[-1]
                if 'PARAM' in {last1, last2} and any(
                    x in {last1, last2} for x in ['clear', 'cancel', 'checkout']
                ):
                    return True

        return False

    def _calculate_validation_compliance(self, found: List[str], expected: List[str], use_exact_matching: bool = False) -> tuple[float, List[str]]:
        """
        Calculate validation compliance with exact or flexible matching

        ARCHITECTURE RULE #1 COMPLIANT: All found validations are counted (no filtering of read_only or other unrecognized types).

        FLEXIBLE MATCHING (use_exact_matching=False):
        - For each expected validation, check if any found validation contains the constraint substring
        - Example: Expected "Product.price: gt=0" matches Found "Product.price: gt=0" (substring match)
        - If all expected validations are found, return compliance=1.0 with ALL found validations as matched
        - This ensures read_only and other validations are properly counted

        EXACT MATCHING (use_exact_matching=True):
        - For each expected validation, check for exact match in found list
        - Used when manual ground truth exists (deprecated per Architecture Rule #1)

        Args:
            found: Validation signatures found in code (e.g., ["Product.price: gt=0", "Product.name: read_only"])
            expected: Validation requirements from spec (e.g., ["Product.price: gt=0", "Product.name: required"])

        Returns:
            Tuple of (compliance_score 0.0-1.0, list of matched validation strings)
        """
        if not expected:
            return 1.0, found  # No expectations, all found validations count

        if not found:
            return 0.0, []  # Nothing found, no matches

        # FLEXIBLE MATCHING MODE (default - per Architecture Rule #1)
        # Use semantic equivalence mapping for common validation patterns
        # Maps high-level constraints from spec to code-level constraints
        # CAMBIO 2: Expanded semantic equivalences with wildcard support and computed fields
        semantic_equivalences = {
            # Database-level constraints
            "unique": ["unique", "read_only", "primary"],
            "primary_key": ["primary_key", "unique"],
            "foreign_key": ["foreign_key*", "fk_*"],  # Matches foreign_key_customer, foreign_key_product, etc.
            "required": ["required"],

            # Auto-generated variants
            "auto-generated": ["default_factory*", "auto_increment", "generated", "auto-generated", "default=uuid.uuid4", "default=datetime.utcnow"],
            "auto_generated": ["default_factory*", "auto_increment", "generated", "auto-generated", "default=uuid.uuid4", "default=datetime.utcnow"],
            "auto-increment": ["default_factory*", "auto_increment"],

            # Read-only variants (output fields, snapshots, immutable)
            "read-only": ["description", "read_only", "exclude", "snapshot_at*"],
            "read_only": ["description", "read_only", "exclude", "snapshot_at*"],
            "snapshot_at": ["description", "read_only", "exclude", "snapshot_at*"],
            "snapshot_at_add_time": ["description", "read_only", "exclude", "default*"],
            "snapshot_at_order_time": ["description", "read_only", "exclude", "default*"],
            "immutable": ["description", "read_only", "exclude"],

            # Computed fields (auto-calculated, derived)
            "computed_field": ["description"],
            "auto-calculated": ["description"],
            "auto_calculated": ["description"],

            # Input validation constraints
            "presence": ["required"],
            "non-empty": ["required", "min_length"],
            "non_empty": ["required", "min_length"],
            "non-negative": ["ge=0", "gt=-1", "minimum=0"],
            "non_negative": ["ge=0", "gt=-1", "minimum=0"],
            "positive": ["gt=0", "ge=1"],
            "greater_than_zero": ["gt=0", "gt"],
            "gt": ["gt"],
            "ge": ["ge"],
            "lt": ["lt"],
            "le": ["le"],
            "min_length": ["min_length"],
            "max_length": ["max_length"],

            # Format validations
            "valid_email_format": ["email_format", "pattern"],
            "valid-email-format": ["email_format", "pattern"],
            "email_format": ["email_format", "pattern"],
            "uuid_format": ["uuid_format"],

            # Default values (with wildcard support for specific values)
            "default_true": ["default=True", "default=true", "default_true", "default*true"],
            "default_false": ["default=False", "default=false", "default_false", "default*false"],
            "default_open": ["default=open", "default_open", "default*open"],
            "default_pending_payment": ["default=pending_payment", "default_pending_payment", "default*pending*payment"],
            "default_pending": ["default=pending", "default_pending", "default*pending"],

            # Enum/values constraints
            "values": ["enum", "values", "default*"],  # Enum constraints often have default values

            # Read-only/computed fields (should match description or auto-generated patterns)
            "read-only": ["description", "read_only", "exclude", "snapshot_at*"],
            "auto-calculated": ["description", "auto*calculated"],
        }

        matches = 0
        matched_validations = []
        unmatched_validations = []

        for exp_val in expected:
            # Extract entity.field and constraint from expected
            # Expected format: "Entity.field: constraint" (e.g., "Product.price: gt=0")
            found_match = False

            if ": " in exp_val:
                entity_field, constraint = exp_val.split(": ", 1)
                constraint_lower = constraint.lower()

                for found_val in found:
                    # Check if this is the same entity.field
                    if entity_field not in found_val:
                        continue

                    # Try exact match first (strict)
                    if constraint in found_val:
                        matches += 1
                        matched_validations.append(found_val)
                        found_match = True
                        break

                    # Extract the constraint part from found_val for matching
                    # Found format: "Entity.field: constraint_value"
                    if ": " in found_val:
                        found_constraint = found_val.split(": ", 1)[1].lower()
                    else:
                        found_constraint = found_val.lower()

                    # Try semantic equivalence match with wildcard support
                    # Check if the constraint has semantic equivalents
                    for semantic_key, equivalents in semantic_equivalences.items():
                        if semantic_key in constraint_lower:
                            # Check if any equivalent matches (with wildcard support)
                            for equiv in equivalents:
                                if self._matches_with_wildcard(found_constraint, equiv.lower()):
                                    matches += 1
                                    matched_validations.append(found_val)
                                    found_match = True
                                    break
                            if found_match:
                                break

                    if found_match:
                        break
            else:
                # Fallback: simple substring match for edge cases
                for found_val in found:
                    if exp_val in found_val:
                        matches += 1
                        matched_validations.append(found_val)
                        found_match = True
                        break

            if not found_match:
                unmatched_validations.append(exp_val)

        # Calculate compliance
        if matches == len(expected):
            # All required validations are present
            # Per the new logic: return ALL found validations as matched (includes extras like read_only)
            compliance = 1.0  # 100% - all required are present
            logger.info(f"✅ Validation compliance: All {len(expected)} required validations found + {len(found) - len(expected)} extra validations")
            logger.info(f"   Found validations: {len(found)} total")
            return compliance, found  # Return ALL found as matched (not just matched ones)
        else:
            # Some required validations are missing
            compliance = matches / len(expected)
            logger.warning(f"⚠️ Validation compliance: {matches}/{len(expected)} required validations found = {compliance:.1%}")
            logger.warning(f"❌ UNMATCHED VALIDATIONS ({len(unmatched_validations)}):")
            for i, val in enumerate(unmatched_validations[:10], 1):
                logger.warning(f"  {i}. {val}")
            if len(unmatched_validations) > 10:
                logger.warning(f"  ... and {len(unmatched_validations) - 10} more")
            return compliance, matched_validations



    def _identify_missing_requirements(
        self,
        entities_expected: List[str],
        entities_found: List[str],
        endpoints_expected: List[str],
        endpoints_found: List[str],
        spec: SpecRequirements,
    ) -> List[str]:
        """
        Identify specific missing requirements

        Args:
            entities_expected: Expected entity names
            entities_found: Found entity names
            endpoints_expected: Expected endpoints
            endpoints_found: Found endpoints
            spec: Full spec requirements

        Returns:
            List of missing requirement descriptions
        """
        missing = []

        # Missing entities
        entities_found_normalized = {e.lower() for e in entities_found}
        for entity in entities_expected:
            if entity.lower() not in entities_found_normalized:
                missing.append(f"Entity: {entity}")

        # Missing endpoints (sample first 5 to avoid spam)
        endpoints_found_normalized = {e.lower() for e in endpoints_found}
        missing_endpoints = []
        for endpoint in endpoints_expected:
            if endpoint.lower() not in endpoints_found_normalized:
                missing_endpoints.append(f"Endpoint: {endpoint}")

        # Add sample of missing endpoints
        missing.extend(missing_endpoints[:5])
        if len(missing_endpoints) > 5:
            missing.append(f"... and {len(missing_endpoints) - 5} more endpoints")

        # Missing functional requirements (sample)
        for req in spec.requirements[:5]:
            if req.type == "functional":
                # Check if requirement keywords are in code (heuristic)
                # This is a simplified check
                missing.append(f"Requirement {req.id}: {req.description[:60]}...")

        return missing

    def _format_entity_report(
        self,
        report: ComplianceReport
    ) -> str:
        """
        Enhanced entity report with categorization

        Separates:
        - Domain entities (Product, Customer, etc.)
        - Request/Response schemas (ProductCreate, etc.)
        - Enums (Status enums)

        Args:
            report: ComplianceReport with entities data

        Returns:
            Formatted string with categorized entities

        Part of Task Group 2: Presentation Enhancement (M4)
        """
        # Categorize entities
        domain_entities = []
        schemas = []
        enums = []

        for entity in report.entities_implemented:
            if entity in report.entities_expected:
                domain_entities.append(entity)
            elif entity.endswith(('Create', 'Update', 'Request', 'Response')):
                schemas.append(entity)
            elif entity.endswith('Status'):
                enums.append(entity)
            else:
                # Unknown category - treat as domain entity
                domain_entities.append(entity)

        # Format output
        lines = []
        lines.append(f"\n📦 Entities ({len(report.entities_expected)} required, {len(domain_entities)} present):")
        lines.append(f"   ✅ {', '.join(domain_entities)}")

        if schemas or enums:
            lines.append(f"\n   📝 Additional models (best practices):")
            if schemas:
                lines.append(f"   - Request/Response schemas: {len(schemas)}")
            if enums:
                lines.append(f"   - Enums: {len(enums)}")

        return "\n".join(lines)

    def _format_endpoint_report(
        self,
        report: ComplianceReport
    ) -> str:
        """
        Enhanced endpoint report with categorization

        Separates:
        - Required endpoints (from spec)
        - Additional endpoints (best practices like / and /health)

        Args:
            report: ComplianceReport with endpoints data

        Returns:
            Formatted string with categorized endpoints

        Part of Task Group 2: Presentation Enhancement (M4)
        """
        # Categorize endpoints
        required_endpoints = []
        additional_endpoints = []

        for endpoint in report.endpoints_implemented:
            if endpoint in report.endpoints_expected:
                required_endpoints.append(endpoint)
            else:
                # Additional endpoints (best practices)
                additional_endpoints.append(endpoint)

        # Format output
        lines = []
        lines.append(f"\n🌐 Endpoints ({len(report.endpoints_expected)} required, {len(required_endpoints)} present):")
        
        if required_endpoints:
            # Show sample of required endpoints (first 5)
            sample = required_endpoints[:5]
            lines.append(f"   ✅ {', '.join(sample)}")
            if len(required_endpoints) > 5:
                lines.append(f"   ... and {len(required_endpoints) - 5} more")
        else:
            lines.append(f"   ❌ No required endpoints found")

        if additional_endpoints:
            lines.append(f"\n   📝 Additional endpoints (best practices):")
            for endpoint in additional_endpoints:
                lines.append(f"   - {endpoint}")

        return "\n".join(lines)

    def generate_detailed_report(
        self, spec_requirements: SpecRequirements, generated_code: str
    ) -> Dict[str, Any]:
        """
        Generate comprehensive compliance report with all details

        Args:
            spec_requirements: Parsed specification
            generated_code: Generated code

        Returns:
            Dict with full compliance analysis
        """
        report = self.validate(spec_requirements, generated_code)
        stats = self.analyzer.get_code_statistics(generated_code)

        detailed_report = {
            "compliance": {
                "overall": report.overall_compliance,
                "entities": report.compliance_details["entities"],
                "endpoints": report.compliance_details["endpoints"],
                "validations": report.compliance_details["validations"],
            },
            "entities": {
                "expected": report.entities_expected,
                "implemented": report.entities_implemented,
                "missing": [
                    e for e in report.entities_expected if e not in report.entities_implemented
                ],
            },
            "endpoints": {
                "expected_count": len(report.endpoints_expected),
                "implemented_count": len(report.endpoints_implemented),
                "sample_expected": report.endpoints_expected[:5],
                "sample_implemented": report.endpoints_implemented[:5],
            },
            "validations": {
                "expected_count": len(report.validations_expected),
                "implemented_count": len(report.validations_implemented),
                "types_found": report.validations_implemented,
            },
            "code_statistics": stats,
            "missing_requirements": report.missing_requirements,
            "passed": report.overall_compliance >= 0.80,
        }

        return detailed_report